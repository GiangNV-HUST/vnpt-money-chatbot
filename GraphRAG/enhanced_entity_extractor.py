# -*- coding: utf-8 -*-
"""
Enhanced Entity Extractor v·ªõi Regex & Confidence Scoring
C·∫£i ti·∫øn t·ª´ SimpleEntityExtractor
"""

import re
import logging
from typing import Dict, List, Tuple
from simple_entity_extractor import SimpleEntityExtractor

# Setup logger
logger = logging.getLogger(__name__)


class EnhancedEntityExtractor(SimpleEntityExtractor):
    """
    Enhanced version v·ªõi:
    1. Regex patterns cho flexibility
    2. Confidence scoring
    3. Validation layer
    """

    def __init__(self):
        super().__init__()

        # TH√äM: Regex patterns cho error detection
        # IMPORTANT: Error names MUST match Neo4j Error node names exactly!
        self.error_patterns_regex = [
            # "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn" - MAIN ERROR for transfer/payment issues
            # Pattern 1: Direct "kh√¥ng nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"
            (r"(ch∆∞a|kh√¥ng|ko)\s+(nh·∫≠n|c√≥|th·∫•y)\s+(ƒë∆∞·ª£c\s+)?ti·ªÅn", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),
            # Pattern 2: "kh√¥ng nh·∫≠n ƒë∆∞·ª£c" + context about recipient/phone
            (r"(thu√™ bao|ƒëi·ªán tho·∫°i|t√†i kho·∫£n|ng∆∞·ªùi nh·∫≠n|b√™n nh·∫≠n|th·ª• h∆∞·ªüng).*(ch∆∞a|kh√¥ng|ko)\s+(nh·∫≠n|c√≥|th·∫•y)\s+(ƒë∆∞·ª£c)?", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),
            # Pattern 3: "n·∫°p/chuy·ªÉn/r√∫t ti·ªÅn" + "kh√¥ng nh·∫≠n ƒë∆∞·ª£c"
            (r"(n·∫°p|chuy·ªÉn|r√∫t)\s+(ti·ªÅn|kho·∫£n).*(ch∆∞a|kh√¥ng|ko)\s+(nh·∫≠n|c√≥|th·∫•y)\s+(ƒë∆∞·ª£c)?", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),
            # Pattern 4: Existing pattern for "ti·ªÅn ch∆∞a v·ªÅ"
            (r"ti·ªÅn\s+(ch∆∞a|kh√¥ng|ko)\s+(v·ªÅ|t·ªõi|ƒë·∫øn)", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),
            # Pattern 5: "th√†nh c√¥ng nh∆∞ng kh√¥ng nh·∫≠n ƒë∆∞·ª£c" (for your specific case)
            (r"(th√†nh c√¥ng|ƒë√£.*tr·ª´).*(nh∆∞ng|m√†).*(ch∆∞a|kh√¥ng)\s+(nh·∫≠n|c√≥|th·∫•y)\s+(ƒë∆∞·ª£c)?", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),

            # "Th√¥ng tin giao d·ªãch kh√¥ng nh·∫≠n ƒë∆∞·ª£c" - For when transaction shows success but recipient didn't receive
            # This matches FAQ_72! Priority: Check this BEFORE generic patterns
            (r"(n·∫°p|chuy·ªÉn).*(th√†nh c√¥ng).*(thu√™ bao|ƒëi·ªán tho·∫°i|ng∆∞·ªùi nh·∫≠n|b√™n nh·∫≠n).*(kh√¥ng|ch∆∞a)\s+nh·∫≠n", "Th√¥ng tin giao d·ªãch kh√¥ng nh·∫≠n ƒë∆∞·ª£c"),
            # Alternative pattern: "th√†nh c√¥ng" but carrier/bank confirms not received
            (r"(th√†nh c√¥ng).*(nh√† m·∫°ng|ng√¢n h√†ng).*(kh·∫≥ng ƒë·ªãnh|x√°c nh·∫≠n).*(kh√¥ng|ch∆∞a)", "Th√¥ng tin giao d·ªãch kh√¥ng nh·∫≠n ƒë∆∞·ª£c"),

            # Giao d·ªãch th·∫•t b·∫°i patterns
            (r"giao\s*d·ªãch\s+(th·∫•t\s*b·∫°i|l·ªói|fail)", "giao d·ªãch th·∫•t b·∫°i"),
            (r"(kh√¥ng|ch∆∞a)\s+(chuy·ªÉn|n·∫°p|r√∫t)\s+ƒë∆∞·ª£c", "giao d·ªãch th·∫•t b·∫°i"),
            (r"(kh√¥ng|ch∆∞a)\s+(th√†nh c√¥ng)", "giao d·ªãch th·∫•t b·∫°i"),
            # Pattern: "ƒë√£ tr·ª´ ti·ªÅn nh∆∞ng ... ch∆∞a c√≥/nh·∫≠n"
            (r"(ƒë√£|b·ªã)\s+tr·ª´\s+ti·ªÅn.*(ch∆∞a|kh√¥ng)\s+(c√≥|nh·∫≠n)", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),
            (r"(ch∆∞a|kh√¥ng)\s+(c√≥|nh·∫≠n).*(ƒë√£|b·ªã)\s+tr·ª´\s+ti·ªÅn", "t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),

            # ƒêang x·ª≠ l√Ω nh∆∞ng ƒë√£ tr·ª´ ti·ªÅn (CRITICAL)
            (r"(ƒëang\s+x·ª≠\s+l√Ω|processing).*(ƒë√£|b·ªã).*(tr·ª´|m·∫•t)\s+ti·ªÅn", "giao d·ªãch ƒëang x·ª≠ l√Ω nh∆∞ng ƒë√£ tr·ª´ ti·ªÅn"),
            (r"(ƒë√£|b·ªã).*(tr·ª´|m·∫•t)\s+ti·ªÅn.*(ƒëang\s+x·ª≠\s+l√Ω|processing)", "giao d·ªãch ƒëang x·ª≠ l√Ω nh∆∞ng ƒë√£ tr·ª´ ti·ªÅn"),
            (r"(n·∫°p|chuy·ªÉn|r√∫t)\s+ti·ªÅn.*(ƒëang\s+x·ª≠\s+l√Ω|b√°o.*ƒëang).*(ƒë√£|b·ªã)\s+tr·ª´", "giao d·ªãch ƒëang x·ª≠ l√Ω nh∆∞ng ƒë√£ tr·ª´ ti·ªÅn"),
            (r"tr·∫°ng\s+th√°i.*(ƒëang\s+x·ª≠\s+l√Ω).*(ƒë√£|b·ªã)\s+tr·ª´", "giao d·ªãch ƒëang x·ª≠ l√Ω nh∆∞ng ƒë√£ tr·ª´ ti·ªÅn"),

            # OTP patterns
            (r"(kh√¥ng|ch∆∞a)\s+nh·∫≠n\s+(ƒë∆∞·ª£c\s+)?(otp|m√£)", "kh√¥ng nh·∫≠n ƒë∆∞·ª£c OTP"),

            # Account/Card invalid
            (r"(th·∫ª|t√†i\s*kho·∫£n)\s+(kh√¥ng\s+)?h·ª£p\s*l·ªá", "th√¥ng tin th·∫ª/t√†i kho·∫£n kh√¥ng h·ª£p l·ªá"),

            # Over limit
            (r"(qu√°|v∆∞·ª£t)\s+(h·∫°n\s*m·ª©c|gi·ªõi\s*h·∫°n)", "qu√° h·∫°n m·ª©c"),

            # Wrong transfer patterns (CRITICAL for "chuy·ªÉn ti·ªÅn nh·∫ßm")
            (r"(chuy·ªÉn|n·∫°p|r√∫t)\s+(ti·ªÅn\s+)?(nh·∫ßm|sai|l·ª°)", "chuy·ªÉn ti·ªÅn nh·∫ßm"),
            (r"(nh·∫ßm|sai|l·ª°)\s+(chuy·ªÉn|n·∫°p)", "chuy·ªÉn ti·ªÅn nh·∫ßm"),
            (r"chuy·ªÉn\s+nh·∫ßm", "chuy·ªÉn ti·ªÅn nh·∫ßm"),

            # Biometric error patterns (CRITICAL for biometric issues)
            (r"sinh\s*tr·∫Øc\s*(h·ªçc)?.*(b√°o|hi·ªán|th√¥ng\s*b√°o|xu·∫•t\s*hi·ªán).*(l·ªói|sai)", "l·ªói sinh tr·∫Øc h·ªçc"),
            (r"(h·ªá\s*th·ªëng|app).*(b√°o|hi·ªán|th√¥ng\s*b√°o).*(l·ªói|sai).*(sinh\s*tr·∫Øc|ekyc)", "l·ªói sinh tr·∫Øc h·ªçc"),
            (r'b√°o\s*l·ªói.*[\"\'].*ng√†y\s*sinh.*[\"\']', "l·ªói ng√†y sinh kh√¥ng kh·ªõp"),
            (r'b√°o\s*l·ªói.*[\"\'].*ƒë·ªãnh\s*danh.*[\"\']', "l·ªói ƒë·ªãnh danh kh√¥ng kh·ªõp"),
            (r"ng√†y\s*sinh.*(kh√¥ng\s*kh·ªõp|kh√¥ng\s*ƒë√∫ng|sai)", "l·ªói ng√†y sinh kh√¥ng kh·ªõp"),
            (r"ƒë·ªãnh\s*danh.*(kh√¥ng\s*kh·ªõp|kh√¥ng\s*ƒë√∫ng|sai)", "l·ªói ƒë·ªãnh danh kh√¥ng kh·ªõp"),
            (r"(cmnd|cccd|cƒÉn\s*c∆∞·ªõc).*(kh√¥ng\s*kh·ªõp|kh√¥ng\s*ƒë√∫ng|sai)", "l·ªói ƒë·ªãnh danh kh√¥ng kh·ªõp"),
            (r"(sinh\s*tr·∫Øc|ekyc).*(th·∫•t\s*b·∫°i|kh√¥ng\s*th√†nh\s*c√¥ng)", "sinh tr·∫Øc h·ªçc th·∫•t b·∫°i"),
            (r"(kh√¥ng|ch∆∞a).*(sinh\s*tr·∫Øc|ekyc).*(ƒë∆∞·ª£c|th√†nh\s*c√¥ng)", "sinh tr·∫Øc h·ªçc th·∫•t b·∫°i"),
        ]

        # TH√äM: Regex patterns cho topic detection
        self.topic_patterns_regex = [
            # CRITICAL FIX: Thanh to√°n h√≥a ƒë∆°n patterns (SPECIFIC patterns first!)
            (r"thanh\s*to√°n\s+(h√≥a\s*ƒë∆°n\s+)?(ti·ªÅn\s+)?(ƒëi·ªán|n∆∞·ªõc|gas)", "Thanh to√°n h√≥a ƒë∆°n"),
            (r"thanh\s*to√°n\s+h√≥a\s*ƒë∆°n\s+(vi·ªÖn\s*th√¥ng|internet|c∆∞·ªõc)", "Thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng"),
            (r"thanh\s*to√°n\s+(c∆∞·ªõc\s+)?(vi·ªÖn\s*th√¥ng|internet)", "Thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng"),
            (r"thanh\s*to√°n\s+h√≥a\s*ƒë∆°n", "Thanh to√°n h√≥a ƒë∆°n"),

            # Generic topics that LLM often extracts
            (r"chuy·ªÉn\s+ti·ªÅn", "Chuy·ªÉn ti·ªÅn"),
            (r"n·∫°p\s+ti·ªÅn", "N·∫°p ti·ªÅn"),
            (r"r√∫t\s+ti·ªÅn", "R√∫t ti·ªÅn"),
            (r"c√†i\s+ƒë·∫∑t", "C√†i ƒë·∫∑t"),
            (r"h·ªßy\s+d·ªãch\s+v·ª•", "H·ªßy d·ªãch v·ª•"),
            (r"m·ªü\s+t√†i\s+kho·∫£n", "M·ªü t√†i kho·∫£n"),
            (r"ƒë·ªãnh\s+danh", "ƒê·ªãnh danh"),
            (r"li√™n\s+k·∫øt\s+ng√¢n\s+h√†ng", "Li√™n k·∫øt ng√¢n h√†ng"),
            # Context-based Topic patterns
            (r"nh·∫≠p\s+s·ªë\s+ti·ªÅn\s+c·∫ßn\s+chuy·ªÉn", "Chuy·ªÉn ti·ªÅn"),  # From context

            # M·ªü kh√≥a patterns
            (r"m·ªü\s*kh√≥a\s+(t√†i\s*kho·∫£n|v√≠)", "M·ªü kh√≥a t√†i kho·∫£n"),
            (r"unlock\s+(account|v√≠)", "M·ªü kh√≥a t√†i kho·∫£n"),

            # Kh√≥a patterns
            (r"kh√≥a\s+(t√†i\s*kho·∫£n|v√≠)", "Kh√≥a t√†i kho·∫£n"),
            (r"lock\s+(account|v√≠)", "Kh√≥a t√†i kho·∫£n"),

            # ƒê·ªïi m·∫≠t kh·∫©u patterns
            (r"ƒë·ªïi\s+(m·∫≠t\s*kh·∫©u|password)", "ƒê·ªïi m·∫≠t kh·∫©u"),
            (r"thay\s*ƒë·ªïi\s+(m·∫≠t\s*kh·∫©u|password)", "ƒê·ªïi m·∫≠t kh·∫©u"),
            (r"ƒë·∫∑t\s*l·∫°i\s+(m·∫≠t\s*kh·∫©u|password)", "ƒê·ªïi m·∫≠t kh·∫©u"),
            (r"change\s+password", "ƒê·ªïi m·∫≠t kh·∫©u"),
        ]

        # TH√äM: Regex patterns cho Action detection (NEW!)
        self.action_patterns_regex = [
            (r"h·ªßy\s+(li√™n\s*k·∫øt)", "H·ªßy li√™n k·∫øt"),
            (r"li√™n\s*k·∫øt\s+(l·∫°i|l·∫°i\s+ng√¢n\s*h√†ng)", "Li√™n k·∫øt l·∫°i"),
            (r"ki·ªÉm\s*tra\s+tr·∫°ng\s*th√°i", "Ki·ªÉm tra tr·∫°ng th√°i"),
            (r"tra\s*c·ª©u\s+tr·∫°ng\s*th√°i", "Ki·ªÉm tra tr·∫°ng th√°i"),
            (r"li√™n\s*h·ªá\s+(h·ªó\s*tr·ª£|hotline)", "Li√™n h·ªá h·ªó tr·ª£"),
            (r"g·ªçi\s+hotline", "Li√™n h·ªá h·ªó tr·ª£"),
            (r"tra\s*so√°t", "Tra so√°t"),
            (r"y√™u\s*c·∫ßu\s+tra\s*so√°t", "Tra so√°t"),

            # Common actions from LLM gaps
            (r"m·ªü\s+t√†i\s+kho·∫£n", "M·ªü t√†i kho·∫£n"),
            (r"xem\s+l·ªãch\s+s·ª≠\s+giao\s+d·ªãch", "Xem l·ªãch s·ª≠ giao d·ªãch"),
            (r"b·∫•m\s+v√†o\s+n√∫t\s+x√°c\s+nh·∫≠n", "B·∫•m v√†o n√∫t X√°c nh·∫≠n"),
        ]

        # TH√äM: Regex patterns cho Status detection (NEW!)
        self.status_patterns_regex = [
            (r"ƒëang\s+x·ª≠\s+l√Ω", "ƒêang x·ª≠ l√Ω"),
            (r"ch·ªù\s+x√°c\s+nh·∫≠n", "Ch·ªù x√°c nh·∫≠n"),
            (r"th·∫•t\s+b·∫°i", "Th·∫•t b·∫°i"),
            (r"th√†nh\s+c√¥ng", "Th√†nh c√¥ng"),
        ]

        # TH√äM: Regex patterns cho Fee detection (NEW! - IMPORTANT)
        self.fee_patterns_regex = [
            (r"ph√≠\s+(r√∫t\s+ti·ªÅn|chuy·ªÉn\s+ti·ªÅn|n·∫°p\s+ti·ªÅn|d·ªãch\s+v·ª•)", "ph√≠"),
            (r"bi·ªÉu\s+ph√≠", "bi·ªÉu ph√≠"),
            (r"b·∫£ng\s+ph√≠", "b·∫£ng ph√≠"),
            (r"m·∫•t\s+ph√≠", "ph√≠"),
            (r"t√≠nh\s+ph√≠", "ph√≠"),
        ]

        # TH√äM: Regex patterns cho Limit detection (NEW!)
        self.limit_patterns_regex = [
            (r"h·∫°n\s+m·ª©c", "h·∫°n m·ª©c"),
            (r"gi·ªõi\s+h·∫°n", "gi·ªõi h·∫°n"),
            (r"s·ªë\s+ti·ªÅn\s+t·ªëi\s+ƒëa", "h·∫°n m·ª©c"),
            (r"t·ªëi\s+ƒëa", "t·ªëi ƒëa"),
        ]

        # TH√äM: Regex patterns cho Requirement detection (NEW! - PRIORITY 1)
        self.requirement_patterns_regex = [
            (r"c·∫ßn\s+(c√≥|g√¨|ph·∫£i|l√†m|nh·ªØng g√¨)", "ƒëi·ªÅu ki·ªán c·∫ßn thi·∫øt"),
            (r"y√™u\s*c·∫ßu", "y√™u c·∫ßu"),
            (r"ph·∫£i\s+c√≥", "ƒëi·ªÅu ki·ªán"),
            (r"ƒëi·ªÅu\s*ki·ªán", "ƒëi·ªÅu ki·ªán"),
            (r"b·∫Øt\s+bu·ªôc", "b·∫Øt bu·ªôc"),
            (r"c·∫ßn\s+thi·∫øt", "c·∫ßn thi·∫øt"),
            (r"ƒë√≤i\s+h·ªèi", "y√™u c·∫ßu"),
            (r"c√≥\s+(cccd|cmnd)", "C√≥ CCCD"),  # From LLM gaps
        ]

        # TH√äM: Regex patterns cho Feature detection (NEW! - PRIORITY 1)
        self.feature_patterns_regex = [
            (r"\bqr\b", "QR"),
            (r"m√£\s+qr", "QR"),
            (r"\bnfc\b", "NFC"),
            (r"\botp\b", "OTP"),
            (r"m√£\s+otp", "OTP"),
            (r"tra\s*so√°t", "tra so√°t"),
            (r"l·ªãch\s+s·ª≠\s+(giao\s+d·ªãch|n·∫°p\s+ti·ªÅn)", "l·ªãch s·ª≠ giao d·ªãch"),
            (r"sinh\s*tr·∫Øc\s*h·ªçc", "sinh tr·∫Øc h·ªçc"),
            (r"ekyc", "eKYC"),
            (r"x√°c\s+th·ª±c\s+2\s+l·ªõp", "2FA"),
            (r"2fa", "2FA"),
            (r"th√¥ng\s+b√°o\s+push", "push notification"),
            (r"v√¢n\s+tay", "v√¢n tay"),
            (r"khu√¥n\s+m·∫∑t", "nh·∫≠n di·ªán khu√¥n m·∫∑t"),
        ]

        # TH√äM: Regex patterns cho UIElement detection (NEW! - PRIORITY 1 - CRITICAL!)
        self.ui_element_patterns_regex = [
            # Menu tabs
            (r"(tab|m·ª•c)\s+c√°\s+nh√¢n", "C√° nh√¢n"),
            (r"(tab|m·ª•c)\s+ng√¢n\s+h√†ng\s+li√™n\s+k·∫øt", "Ng√¢n h√†ng li√™n k·∫øt"),
            (r"(tab|m·ª•c)\s+chuy·ªÉn\s+ti·ªÅn", "Chuy·ªÉn ti·ªÅn"),
            (r"(tab|m·ª•c)\s+n·∫°p\s+ti·ªÅn", "N·∫°p ti·ªÅn"),
            (r"(tab|m·ª•c)\s+r√∫t\s+ti·ªÅn", "R√∫t ti·ªÅn"),
            (r"(tab|m·ª•c)\s+thanh\s+to√°n", "Thanh to√°n"),
            (r"(tab|m·ª•c)\s+l·ªãch\s+s·ª≠", "L·ªãch s·ª≠"),
            (r"(tab|m·ª•c)\s+c√†i\s+ƒë·∫∑t", "C√†i ƒë·∫∑t"),
            (r"(tab|m·ª•c)\s+tr·ª£\s+gi√∫p", "Tr·ª£ gi√∫p"),

            # Buttons (specific)
            (r"n√∫t\s+x√°c\s+nh·∫≠n", "X√°c nh·∫≠n"),  # From LLM gaps
            (r"n√∫t\s+(chuy·ªÉn\s+ti·ªÅn|n·∫°p\s+ti·ªÅn|r√∫t\s+ti·ªÅn|x√°c\s+nh·∫≠n|h·ªßy)", "n√∫t"),
            (r"(nh·∫•n|b·∫•m|ch·ªçn)\s+v√†o", "action button"),

            # Generic references
            (r"\bm·ª•c\b", "M·ª•c"),  # From LLM gaps

            # Fields
            (r"(√¥|tr∆∞·ªùng)\s+nh·∫≠p", "input field"),
            (r"nh·∫≠p\s+(s·ªë\s+ti·ªÅn|s·ªë\s+ƒëi·ªán\s+tho·∫°i|t√†i\s+kho·∫£n)", "input field"),

            # Icons
            (r"bi·ªÉu\s+t∆∞·ª£ng", "icon"),
            (r"icon", "icon"),
        ]

        # TH√äM: Regex patterns cho TimeFrame detection (NEW! - PRIORITY 2)
        self.timeframe_patterns_regex = [
            (r"ng√†y\s+l√†m\s+vi·ªác", "ng√†y l√†m vi·ªác"),
            (r"ngay\s+l·∫≠p\s+t·ª©c", "ngay l·∫≠p t·ª©c"),
            (r"trong\s+v√≤ng\s+\d+\s+(ng√†y|gi·ªù|ph√∫t)", "trong v√≤ng"),
            (r"24\s*\/\s*7", "24/7"),
            (r"(h√†ng\s+ng√†y|m·ªói\s+ng√†y)", "h√†ng ng√†y"),
            (r"cu·ªëi\s+tu·∫ßn", "cu·ªëi tu·∫ßn"),
            (r"(th·ª©\s+[2-7]|ch·ªß\s+nh·∫≠t)", "ng√†y trong tu·∫ßn"),
            (r"(s√°ng|chi·ªÅu|t·ªëi)", "khung gi·ªù trong ng√†y"),
        ]

        # TH√äM: Regex patterns cho Document detection (NEW! - PRIORITY 2)
        self.document_patterns_regex = [
            (r"\bcccd\b", "CCCD"),
            (r"cƒÉn\s+c∆∞·ªõc\s+c√¥ng\s+d√¢n", "CCCD"),
            (r"cccd\s+g·∫Øn\s+chip", "CCCD g·∫Øn chip"),
            (r"\bcmnd\b", "CMND"),
            (r"ch·ª©ng\s+minh\s+nh√¢n\s+d√¢n", "CMND"),
            (r"h·ªô\s+chi·∫øu", "H·ªô chi·∫øu"),
            (r"passport", "H·ªô chi·∫øu"),
            (r"gi·∫•y\s+t·ªù\s+t√πy\s+th√¢n", "gi·∫•y t·ªù t√πy th√¢n"),
            (r"b·∫±ng\s+l√°i\s+xe", "b·∫±ng l√°i xe"),
        ]

        # TH√äM: Regex patterns cho AccountType detection (NEW! - PRIORITY 2)
        self.account_type_patterns_regex = [
            (r"t√†i\s+kho·∫£n\s+v√≠", "t√†i kho·∫£n v√≠"),
            (r"v√≠\s+ƒëi·ªán\s+t·ª≠", "v√≠ ƒëi·ªán t·ª≠"),
            (r"t√†i\s+kho·∫£n\s+ng√¢n\s+h√†ng", "t√†i kho·∫£n ng√¢n h√†ng"),
            (r"th·∫ª\s+n·ªôi\s+ƒë·ªãa", "th·∫ª n·ªôi ƒë·ªãa"),
            (r"th·∫ª\s+atm", "th·∫ª ATM"),
            (r"th·∫ª\s+t√≠n\s+d·ª•ng", "th·∫ª t√≠n d·ª•ng"),
            (r"th·∫ª\s+ghi\s+n·ª£", "th·∫ª ghi n·ª£"),
            (r"t√†i\s+kho·∫£n\s+thanh\s+to√°n", "t√†i kho·∫£n thanh to√°n"),
        ]

        # TH√äM: Regex patterns cho ContactChannel detection (NEW! - PRIORITY 3)
        self.contact_channel_patterns_regex = [
            (r"hotline", "Hotline"),
            (r"(g·ªçi|ƒëi·ªán)\s+(h·ªó\s+tr·ª£|t·ªïng\s+ƒë√†i)", "Hotline"),
            (r"tr·ª£\s+gi√∫p", "Tr·ª£ gi√∫p"),
            (r"h·ªó\s+tr·ª£\s+tr·ª±c\s+tuy·∫øn", "h·ªó tr·ª£ tr·ª±c tuy·∫øn"),
            (r"chat\s+(h·ªó\s+tr·ª£|support)", "chat support"),
            (r"email\s+(h·ªó\s+tr·ª£|support)", "email support"),
            (r"trung\s+t√¢m\s+h·ªó\s+tr·ª£", "trung t√¢m h·ªó tr·ª£"),
        ]

        # TH√äM: Expanded Service patterns (IMPROVE EXISTING - specific patterns)
        self.service_patterns_regex = [
            (r"vnpt\s+money", "VNPT Money"),
            (r"vnpt\s+pay", "VNPT Pay"),
            (r"mobile\s+banking", "Mobile Banking"),
            (r"internet\s+banking", "Internet Banking"),
            (r"v√≠\s+ƒëi·ªán\s+t·ª≠", "V√≠ ƒëi·ªán t·ª≠"),
            (r"app\s+vnpt", "VNPT Money"),
            (r"·ª©ng\s+d·ª•ng\s+vnpt", "VNPT Money"),
        ]

        # TH√äM: Expanded Bank patterns (IMPROVE EXISTING - specific patterns for major banks)
        self.bank_patterns_regex = [
            (r"vietinbank", "Vietinbank"),
            (r"vietcombank", "Vietcombank"),
            (r"\bbidv\b", "BIDV"),
            (r"techcombank", "Techcombank"),
            (r"\bacb\b", "ACB"),
            (r"vpbank", "VPBank"),
            (r"mb\s+bank", "MB Bank"),
            (r"sacombank", "Sacombank"),
            (r"\bvib\b", "VIB"),
            (r"hdbank", "HDBank"),
            (r"tpbank", "TPBank"),
            (r"\bocb\b", "OCB"),
            (r"agribank", "Agribank"),
            (r"seabank", "SeABank"),
            (r"lienvietpostbank", "LienVietPostBank"),
            (r"\bshb\b", "SHB"),
        ]

        # TH√äM: Expanded Action patterns (IMPROVE EXISTING - more common actions)
        # Note: self.action_patterns_regex already exists, so we'll add to it
        self.action_patterns_regex.extend([
            (r"nh·∫≠p\s+s·ªë\s+ti·ªÅn", "Nh·∫≠p s·ªë ti·ªÅn"),
            (r"ch·ªçn\s+ng√¢n\s+h√†ng", "Ch·ªçn ng√¢n h√†ng"),
            (r"ch·ªçn\s+(lo·∫°i|d·ªãch\s+v·ª•)", "Ch·ªçn d·ªãch v·ª•"),
            (r"x√°c\s+nh·∫≠n\s+giao\s+d·ªãch", "X√°c nh·∫≠n giao d·ªãch"),
            (r"nh·∫≠p\s+m√£\s+otp", "Nh·∫≠p m√£ OTP"),
            (r"nh·∫≠p\s+s·ªë\s+ƒëi·ªán\s+tho·∫°i", "Nh·∫≠p s·ªë ƒëi·ªán tho·∫°i"),
            (r"nh·∫≠p\s+t√†i\s+kho·∫£n", "Nh·∫≠p t√†i kho·∫£n"),
            (r"b·∫•m\s+(n√∫t|ch·ªçn)", "B·∫•m n√∫t"),
            (r"v√†o\s+m·ª•c", "V√†o m·ª•c"),
            (r"chuy·ªÉn\s+sang\s+(tab|m·ª•c)", "Chuy·ªÉn tab"),
        ])

        # TH√äM: Contextual rules
        self.contextual_rules = {
            # N·∫øu c√≥ "chuy·ªÉn ti·ªÅn" + "ch∆∞a" ‚Üí Error: ch∆∞a nh·∫≠n ƒë∆∞·ª£c
            ("chuy·ªÉn ti·ªÅn", "ch∆∞a"): ("Error", "ch∆∞a nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"),
            ("n·∫°p ti·ªÅn", "th·∫•t b·∫°i"): ("Error", "giao d·ªãch th·∫•t b·∫°i"),

            # N·∫øu c√≥ "m·ªü kh√≥a" + "t√†i kho·∫£n" ‚Üí Topic: M·ªü kh√≥a t√†i kho·∫£n
            ("m·ªü kh√≥a", "t√†i kho·∫£n"): ("Topic", "M·ªü kh√≥a t√†i kho·∫£n"),
            ("m·ªü kh√≥a", "v√≠"): ("Topic", "M·ªü kh√≥a t√†i kho·∫£n"),
        }

    def extract_with_confidence(self, query: str) -> Tuple[Dict[str, List[str]], float]:
        """
        HYBRID extraction with strategy selection:
        - LLM-First Strategy (if USE_LLM_FIRST_STRATEGY = True): Always use LLM, regex for augmentation
        - Pattern-First Strategy (default): Pattern first, LLM fallback

        Returns:
            (entities_dict, confidence_score)
        """
        import config

        # Check which strategy to use
        if getattr(config, 'USE_LLM_FIRST_STRATEGY', False):
            # NEW: LLM-First Strategy (80% LLM, 20% Regex)
            return self._extract_llm_first(query)
        else:
            # OLD: Pattern-First Strategy (Regex first, LLM fallback)
            return self._extract_pattern_first(query)

    def _extract_pattern_first(self, query: str) -> Tuple[Dict[str, List[str]], float]:
        """
        Original Pattern-First Strategy: Pattern-first, LLM fallback
        """
        import config

        # Step 1: Pattern-based extraction (from parent)
        entities = self.extract(query)

        # Step 2: Enhanced regex extraction
        regex_entities = self._extract_with_regex(query)

        # Step 3: Merge results
        entities = self._merge_entities(entities, regex_entities)

        # Step 4: Apply contextual rules
        entities = self._apply_contextual_rules(query, entities)

        # Step 5: Validate
        entities = self._validate_entities(query, entities)

        # Step 6: Calculate confidence
        confidence = self._calculate_confidence(query, entities)

        # Step 7: HYBRID - LLM fallback if needed
        if config.ENABLE_LLM_FALLBACK:
            should_use_llm, reason = self._should_use_llm_fallback(query, entities, confidence)

            if should_use_llm:
                logger.info(f"ü§ñ LLM fallback triggered: {reason}")
                logger.info(f"   Pattern confidence: {confidence:.2%}")

                try:
                    llm_entities = self._extract_with_llm(query)

                    if llm_entities:
                        # Merge LLM results with pattern results (LLM takes priority)
                        entities = self._merge_llm_results(entities, llm_entities)
                        confidence = 0.95  # High confidence from LLM
                        logger.info(f"   ‚úÖ LLM extraction successful")
                        logger.info(f"   LLM entities: {llm_entities}")
                except Exception as e:
                    logger.error(f"   ‚ùå LLM fallback failed: {e}")
                    logger.info(f"   ‚Üí Using pattern-based results (confidence: {confidence:.2%})")

        return entities, confidence

    def _extract_llm_first(self, query: str) -> Tuple[Dict[str, List[str]], float]:
        """
        NEW: LLM-First Strategy (80% LLM, 20% Regex)

        Always use LLM for semantic understanding, regex for augmentation/validation
        This provides best accuracy at higher cost
        """
        import config

        logger.info(f"üéØ LLM-First Strategy: Using LLM for primary extraction")

        # Step 1: LLM extraction (PRIMARY - 80% weight)
        try:
            llm_entities = self._extract_with_llm(query)
            llm_success = True
            logger.info(f"   ‚úÖ LLM extraction successful")
            logger.info(f"   LLM entities: {llm_entities}")
        except Exception as e:
            logger.error(f"   ‚ùå LLM extraction failed: {e}")
            logger.info(f"   ‚Üí Falling back to regex-only")
            llm_entities = {}
            llm_success = False

        # Step 2: Regex extraction (SECONDARY - 20% weight, for augmentation)
        regex_entities = self._extract_with_regex(query)
        logger.info(f"   üìã Regex entities (for augmentation): {regex_entities}")

        # Step 3: Intelligent merge (LLM priority)
        if llm_success:
            # Merge with LLM priority (80% LLM, 20% Regex)
            entities = self._merge_llm_priority(llm_entities, regex_entities)
            confidence = 0.95
            logger.info(f"   ‚úÖ Final entities (LLM-first): {entities}")
        else:
            # Fallback to regex-only if LLM fails
            entities = regex_entities
            confidence = self._calculate_confidence(query, entities)
            logger.info(f"   ‚ö†Ô∏è Using regex-only (LLM failed)")

        # Step 4: Validation (optional)
        if getattr(config, 'LLM_FIRST_VALIDATION', True):
            entities = self._validate_with_regex(query, entities)

        return entities, confidence

    def _extract_with_regex(self, query: str) -> Dict[str, List[str]]:
        """Extract entities using regex patterns (EXPANDED for all entity types)"""
        entities = {
            "Error": [],
            "Topic": [],
            "Action": [],
            "Status": [],
            "Fee": [],
            "Limit": [],
            "Requirement": [],
            "Feature": [],
            "UIElement": [],
            "TimeFrame": [],
            "Document": [],
            "AccountType": [],
            "ContactChannel": [],
            "Service": [],
            "Bank": []
        }

        query_lower = query.lower()

        # Check error regex patterns
        for pattern, error_name in self.error_patterns_regex:
            if re.search(pattern, query_lower):
                if error_name not in entities["Error"]:
                    entities["Error"].append(error_name)

        # Check topic regex patterns
        for pattern, topic_name in self.topic_patterns_regex:
            if re.search(pattern, query_lower):
                if topic_name not in entities["Topic"]:
                    entities["Topic"].append(topic_name)

        # Check action regex patterns (NEW!)
        for pattern, action_name in self.action_patterns_regex:
            if re.search(pattern, query_lower):
                if action_name not in entities["Action"]:
                    entities["Action"].append(action_name)

        # Check status regex patterns (NEW!)
        for pattern, status_name in self.status_patterns_regex:
            if re.search(pattern, query_lower):
                if status_name not in entities["Status"]:
                    entities["Status"].append(status_name)

        # Check fee regex patterns (NEW! - IMPORTANT)
        for pattern, fee_name in self.fee_patterns_regex:
            if re.search(pattern, query_lower):
                if fee_name not in entities["Fee"]:
                    entities["Fee"].append(fee_name)

        # Check limit regex patterns (NEW!)
        for pattern, limit_name in self.limit_patterns_regex:
            if re.search(pattern, query_lower):
                if limit_name not in entities["Limit"]:
                    entities["Limit"].append(limit_name)

        # Check requirement regex patterns (NEW! - PRIORITY 1)
        for pattern, requirement_name in self.requirement_patterns_regex:
            if re.search(pattern, query_lower):
                if requirement_name not in entities["Requirement"]:
                    entities["Requirement"].append(requirement_name)

        # Check feature regex patterns (NEW! - PRIORITY 1)
        for pattern, feature_name in self.feature_patterns_regex:
            if re.search(pattern, query_lower):
                if feature_name not in entities["Feature"]:
                    entities["Feature"].append(feature_name)

        # Check UI element regex patterns (NEW! - PRIORITY 1 - CRITICAL!)
        for pattern, ui_name in self.ui_element_patterns_regex:
            if re.search(pattern, query_lower):
                if ui_name not in entities["UIElement"]:
                    entities["UIElement"].append(ui_name)

        # Check timeframe regex patterns (NEW! - PRIORITY 2)
        for pattern, timeframe_name in self.timeframe_patterns_regex:
            if re.search(pattern, query_lower):
                if timeframe_name not in entities["TimeFrame"]:
                    entities["TimeFrame"].append(timeframe_name)

        # Check document regex patterns (NEW! - PRIORITY 2)
        for pattern, document_name in self.document_patterns_regex:
            if re.search(pattern, query_lower):
                if document_name not in entities["Document"]:
                    entities["Document"].append(document_name)

        # Check account type regex patterns (NEW! - PRIORITY 2)
        for pattern, account_type_name in self.account_type_patterns_regex:
            if re.search(pattern, query_lower):
                if account_type_name not in entities["AccountType"]:
                    entities["AccountType"].append(account_type_name)

        # Check contact channel regex patterns (NEW! - PRIORITY 3)
        for pattern, channel_name in self.contact_channel_patterns_regex:
            if re.search(pattern, query_lower):
                if channel_name not in entities["ContactChannel"]:
                    entities["ContactChannel"].append(channel_name)

        # Check service regex patterns (IMPROVED!)
        for pattern, service_name in self.service_patterns_regex:
            if re.search(pattern, query_lower):
                if service_name not in entities["Service"]:
                    entities["Service"].append(service_name)

        # Check bank regex patterns (IMPROVED!)
        for pattern, bank_name in self.bank_patterns_regex:
            if re.search(pattern, query_lower):
                if bank_name not in entities["Bank"]:
                    entities["Bank"].append(bank_name)

        return entities

    def _merge_entities(
        self,
        entities1: Dict[str, List[str]],
        entities2: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """Merge two entity dicts"""
        merged = entities1.copy()

        for entity_type, values in entities2.items():
            if entity_type not in merged:
                merged[entity_type] = []

            for value in values:
                if value not in merged[entity_type]:
                    merged[entity_type].append(value)

        return merged

    def _apply_contextual_rules(
        self,
        query: str,
        entities: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """Apply contextual rules to improve extraction"""
        query_lower = query.lower()

        for (keyword1, keyword2), (entity_type, entity_value) in self.contextual_rules.items():
            if keyword1 in query_lower and keyword2 in query_lower:
                if entity_type not in entities:
                    entities[entity_type] = []
                if entity_value not in entities[entity_type]:
                    entities[entity_type].append(entity_value)

        return entities

    def _validate_entities(
        self,
        query: str,
        entities: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """Validate and auto-correct entities"""
        query_lower = query.lower()

        # Rule: N·∫øu c√≥ "m·ªü kh√≥a" ‚Üí x√≥a "Kh√≥a t√†i kho·∫£n" (v√¨ "m·ªü kh√≥a" ∆∞u ti√™n h∆°n "kh√≥a")
        if "m·ªü kh√≥a" in query_lower and "M·ªü kh√≥a t√†i kho·∫£n" in entities.get("Topic", []):
            if "Kh√≥a t√†i kho·∫£n" in entities.get("Topic", []):
                entities["Topic"].remove("Kh√≥a t√†i kho·∫£n")

        # Rule: N·∫øu c√≥ "kh√≥a" nh∆∞ng KH√îNG c√≥ "m·ªü" ‚Üí c√≥ th·ªÉ l√† "kh√≥a t√†i kho·∫£n"
        # (logic n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi pattern matching)

        # Rule: N·∫øu query c√≥ "chuy·ªÉn ti·ªÅn" m√† kh√¥ng c√≥ Topic, th√™m v√†o
        if "chuy·ªÉn ti·ªÅn" in query_lower or "chuy·ªÉn kho·∫£n" in query_lower:
            if "Chuy·ªÉn ti·ªÅn" not in entities.get("Topic", []):
                if "Topic" not in entities:
                    entities["Topic"] = []
                entities["Topic"].append("Chuy·ªÉn ti·ªÅn")

        # Rule: N·∫øu query c√≥ "n·∫°p ti·ªÅn" m√† kh√¥ng c√≥ Topic, th√™m v√†o
        # CRITICAL FIX: Only add generic "N·∫°p ti·ªÅn" if no specific "n·∫°p ti·ªÅn" topic exists
        if "n·∫°p ti·ªÅn" in query_lower or "n·∫°p" in query_lower:
            existing_topics = entities.get("Topic", [])
            # Check if any existing topic already contains "n·∫°p ti·ªÅn" (more specific)
            has_specific_nap_tien = any("n·∫°p ti·ªÅn" in t.lower() or "n·∫°p" in t.lower()
                                       for t in existing_topics if t != "N·∫°p ti·ªÅn")

            # Debug log
            if has_specific_nap_tien:
                logger.debug(f"   Skipping generic 'N·∫°p ti·ªÅn' - found specific topic: {existing_topics}")

            if not has_specific_nap_tien and "N·∫°p ti·ªÅn" not in existing_topics:
                if "Topic" not in entities:
                    entities["Topic"] = []
                entities["Topic"].append("N·∫°p ti·ªÅn")

        # Rule: N·∫øu query c√≥ "sinh tr·∫Øc h·ªçc" ho·∫∑c "ekyc" m√† kh√¥ng c√≥ Topic, th√™m v√†o
        if ("sinh tr·∫Øc" in query_lower or "ekyc" in query_lower) and "Sinh tr·∫Øc h·ªçc" not in entities.get("Topic", []):
            if "Topic" not in entities:
                entities["Topic"] = []
            entities["Topic"].append("Sinh tr·∫Øc h·ªçc")

        # Rule: N·∫øu query c√≥ "ƒë·ªïi m·∫≠t kh·∫©u" / "thay ƒë·ªïi m·∫≠t kh·∫©u" m√† kh√¥ng c√≥ Topic, th√™m v√†o
        if ("ƒë·ªïi m·∫≠t kh·∫©u" in query_lower or "thay ƒë·ªïi m·∫≠t kh·∫©u" in query_lower or "ƒë·∫∑t l·∫°i m·∫≠t kh·∫©u" in query_lower) and "ƒê·ªïi m·∫≠t kh·∫©u" not in entities.get("Topic", []):
            if "Topic" not in entities:
                entities["Topic"] = []
            entities["Topic"].append("ƒê·ªïi m·∫≠t kh·∫©u")

        # Rule: N·∫øu c√≥ error v·ªÅ "ch∆∞a nh·∫≠n ƒë∆∞·ª£c" + c√≥ "chuy·ªÉn ti·ªÅn"
        # ‚Üí ƒê·∫£m b·∫£o c√≥ Topic "Chuy·ªÉn ti·ªÅn"
        if entities.get("Error"):
            for error in entities["Error"]:
                if "ch∆∞a nh·∫≠n" in error.lower() and "chuy·ªÉn" in query_lower:
                    if "Topic" not in entities:
                        entities["Topic"] = []
                    if "Chuy·ªÉn ti·ªÅn" not in entities["Topic"]:
                        entities["Topic"].append("Chuy·ªÉn ti·ªÅn")
                # Rule: N·∫øu c√≥ error v·ªÅ sinh tr·∫Øc ‚Üí ƒê·∫£m b·∫£o c√≥ Topic "Sinh tr·∫Øc h·ªçc"
                if ("sinh tr·∫Øc" in error.lower() or "ekyc" in error.lower() or "ng√†y sinh" in error.lower() or "ƒë·ªãnh danh" in error.lower()):
                    if "Topic" not in entities:
                        entities["Topic"] = []
                    if "Sinh tr·∫Øc h·ªçc" not in entities["Topic"]:
                        entities["Topic"].append("Sinh tr·∫Øc h·ªçc")

        return entities

    def _calculate_confidence(
        self,
        query: str,
        entities: Dict[str, List[str]]
    ) -> float:
        """
        Calculate confidence score (0.0 - 1.0)

        Scoring:
        - Error found: +0.4
        - Topic found: +0.3
        - Service found: +0.2
        - Multiple entities: +0.1
        """
        confidence = 0.0

        # Error entities are most important for troubleshooting
        if entities.get("Error"):
            confidence += 0.4

        # Topic helps narrow down
        if entities.get("Topic"):
            confidence += 0.3

        # Service provides context
        if entities.get("Service"):
            confidence += 0.2

        # Multiple entities = higher confidence
        total_entities = sum(len(v) for v in entities.values() if isinstance(v, list))
        if total_entities >= 3:
            confidence += 0.1

        return min(confidence, 1.0)  # Cap at 1.0

    def _should_use_llm_fallback(
        self,
        query: str,
        entities: Dict[str, List[str]],
        confidence: float
    ) -> Tuple[bool, str]:
        """
        Determine if LLM fallback is needed

        Returns:
            (should_use_llm: bool, reason: str)
        """
        import config

        query_lower = query.lower()

        # Trigger 1: Low confidence
        if confidence < config.LLM_FALLBACK_THRESHOLD:
            return True, f"low_confidence ({confidence:.2%} < {config.LLM_FALLBACK_THRESHOLD:.2%})"

        # Trigger 2: Question keywords but no relevant answer entities
        if config.LLM_FALLBACK_FOR_QUESTIONS:
            if self._is_question_query(query) and not self._has_answer_entities(entities):
                return True, "question_without_answer_entities"

        # Trigger 3: Ambiguous queries
        if config.LLM_FALLBACK_FOR_AMBIGUOUS:
            if self._is_ambiguous(query, entities):
                return True, "ambiguous_query"

        # Trigger 4: Critical entities missing
        if self._missing_critical_entities(query, entities):
            return True, "missing_critical_entities"

        return False, "pattern_sufficient"

    def _is_question_query(self, query: str) -> bool:
        """Check if query is asking a question"""
        question_keywords = [
            "nh∆∞ th·∫ø n√†o", "l√†m sao", "th·∫ø n√†o",
            "bao nhi√™u", "l√† g√¨", "t·∫°i sao", "khi n√†o",
            "·ªü ƒë√¢u", "ai", "c√≥", "ph·∫£i", "ƒë∆∞·ª£c"
        ]
        query_lower = query.lower()
        return any(kw in query_lower for kw in question_keywords)

    def _has_answer_entities(self, entities: Dict) -> bool:
        """Check if we extracted answer-relevant entities"""
        # Has action, fee, limit, or error entities
        answer_entity_types = ["Action", "Fee", "Limit", "Error", "Status"]
        return any(entities.get(entity_type) for entity_type in answer_entity_types)

    def _is_ambiguous(self, query: str, entities: Dict) -> bool:
        """Check if query/entities are ambiguous"""
        query_lower = query.lower()

        # "h·ªßy" without specific action
        if "h·ªßy" in query_lower and not entities.get("Action"):
            return True

        # Generic pronouns without context
        generic_words = ["n√†y", "ƒë√≥", "kia", "·∫•y"]
        if any(w in query_lower for w in generic_words):
            if len(entities.get("Action", [])) == 0 and len(entities.get("Topic", [])) == 0:
                return True

        return False

    def _missing_critical_entities(self, query: str, entities: Dict) -> bool:
        """Check if critical entities are missing"""
        query_lower = query.lower()

        # Query mentions fee but Fee not extracted
        fee_keywords = ["ph√≠", "bao nhi√™u", "t·ªën", "chi ph√≠", "m·∫•t ti·ªÅn"]
        if any(kw in query_lower for kw in fee_keywords):
            if not entities.get("Fee"):
                return True

        # Query mentions limit but Limit not extracted
        limit_keywords = ["h·∫°n m·ª©c", "t·ªëi ƒëa", "gi·ªõi h·∫°n", "t·ªëi thi·ªÉu"]
        if any(kw in query_lower for kw in limit_keywords):
            if not entities.get("Limit"):
                return True

        # Query mentions action verbs but Action not extracted
        action_keywords = ["l√†m", "th·ª±c hi·ªán", "ti·∫øn h√†nh", "x·ª≠ l√Ω"]
        if any(kw in query_lower for kw in action_keywords):
            if not entities.get("Action"):
                return True

        return False

    def _extract_with_llm(self, query: str) -> Dict[str, List[str]]:
        """
        Extract entities using LLM (same prompt as document extraction)

        Returns:
            Dictionary of entities
        """
        try:
            from llm_entity_extractor import LLMEntityExtractor

            # Initialize LLM extractor (reuse existing instance if possible)
            if not hasattr(self, '_llm_extractor'):
                self._llm_extractor = LLMEntityExtractor()

            # Extract using same method as document extraction
            result = self._llm_extractor.extract_entities_and_relationships(
                question=query,
                answer="",  # Empty for query extraction
                section=""  # Empty for query
            )

            # Return only entities (ignore relationships for query extraction)
            return result.get("entities", {})

        except Exception as e:
            logger.error(f"LLM extraction failed: {e}")
            return {}

    def _merge_llm_results(
        self,
        pattern_entities: Dict[str, List[str]],
        llm_entities: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """
        Merge pattern and LLM results (LLM takes priority)

        Strategy:
        - Keep all LLM entities (they are more accurate)
        - Add pattern entities that LLM didn't find
        - Remove duplicates
        """
        merged = {}

        # Get all entity types from both
        all_types = set(pattern_entities.keys()) | set(llm_entities.keys())

        for entity_type in all_types:
            # Skip special flags
            if entity_type == "out_of_scope":
                merged[entity_type] = pattern_entities.get(entity_type, False)
                continue

            # Start with LLM entities (higher priority)
            llm_vals = llm_entities.get(entity_type, [])
            pattern_vals = pattern_entities.get(entity_type, [])

            # Combine and deduplicate (case-insensitive)
            combined = list(llm_vals)  # Start with LLM

            for pv in pattern_vals:
                # Add pattern value if not already in LLM results
                # CRITICAL FIX: Also skip if pattern value is a substring of any LLM value
                # (e.g., don't add "N·∫°p ti·ªÅn" if LLM found "H·ªßy d·ªãch v·ª• n·∫°p ti·ªÅn t·ª± ƒë·ªông")
                is_duplicate = any(pv.lower() == lv.lower() for lv in llm_vals)
                is_substring = any(pv.lower() in lv.lower() for lv in llm_vals if len(pv) < len(lv))

                if not is_duplicate and not is_substring:
                    combined.append(pv)

            merged[entity_type] = combined

        return merged

    def _merge_llm_priority(
        self,
        llm_entities: Dict[str, List[str]],
        regex_entities: Dict[str, List[str]],
        llm_weight: float = 0.8
    ) -> Dict[str, List[str]]:
        """
        Merge LLM and Regex results with LLM priority (for LLM-First strategy)

        Strategy (80% LLM, 20% Regex):
        - Take all LLM entities (PRIMARY source)
        - Add regex entities only if:
          1. LLM didn't extract that entity type
          2. Regex entity is factual (Bank, Document, Error, Status)
          3. Regex entity provides additional specific detail
        """
        merged = {}

        # Get all entity types
        all_types = set(llm_entities.keys()) | set(regex_entities.keys())

        for entity_type in all_types:
            llm_vals = llm_entities.get(entity_type, [])
            regex_vals = regex_entities.get(entity_type, [])

            # Priority 1: Start with all LLM values (always include)
            merged[entity_type] = llm_vals.copy()

            # Priority 2: Add regex values selectively
            # High-value factual entity types - trust regex
            if entity_type in ['Bank', 'Document', 'Error', 'Status', 'Fee', 'Limit']:
                for rv in regex_vals:
                    if rv not in merged[entity_type]:
                        merged[entity_type].append(rv)

            # If LLM missed entirely, use regex
            elif not llm_vals and regex_vals:
                merged[entity_type].extend(regex_vals)

            # If regex provides more specific detail, add it
            elif regex_vals:
                for rv in regex_vals:
                    # Check if regex value is more specific than LLM value
                    is_more_specific = any(
                        len(rv) > len(lv) and lv.lower() in rv.lower()
                        for lv in llm_vals
                    )
                    # Or if it's completely different valuable info
                    is_different = not any(
                        rv.lower() in lv.lower() or lv.lower() in rv.lower()
                        for lv in llm_vals
                    )

                    if (is_more_specific or is_different) and rv not in merged[entity_type]:
                        merged[entity_type].append(rv)

        return merged

    def _validate_with_regex(
        self,
        query: str,
        entities: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """
        Validate LLM-extracted entities using regex patterns

        Strategy:
        - Remove LLM entities that don't appear in query text (hallucinations)
        - Validate factual entities (Bank, Document) against known patterns
        - Keep semantic entities (Topic, Action) as-is (LLM can infer these)
        """
        validated = {}

        query_lower = query.lower()

        for entity_type, values in entities.items():
            validated[entity_type] = []

            for value in values:
                # Skip validation for semantic entity types (LLM can infer)
                if entity_type in ['Topic', 'Action', 'Requirement', 'Service']:
                    # For these, LLM inference is acceptable
                    # But still check if it's too far from query
                    value_words = set(value.lower().split())
                    query_words = set(query_lower.split())
                    overlap = len(value_words & query_words)

                    # Accept if at least 30% overlap or very short (1-2 words)
                    if overlap > 0 or len(value_words) <= 2:
                        validated[entity_type].append(value)
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Filtered LLM entity (no overlap): {entity_type}={value}")

                # Strict validation for factual entities
                elif entity_type in ['Bank', 'Document', 'Error']:
                    # Must appear in query text
                    if value.lower() in query_lower:
                        validated[entity_type].append(value)
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Filtered LLM entity (not in query): {entity_type}={value}")

                # Moderate validation for others
                else:
                    # Check if at least one word appears
                    value_words = value.lower().split()
                    if any(word in query_lower for word in value_words):
                        validated[entity_type].append(value)
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Filtered LLM entity (no words match): {entity_type}={value}")

        return validated


# ============================================
# TESTING
# ============================================

if __name__ == "__main__":
    import sys
    if sys.platform == 'win32':
        sys.stdout.reconfigure(encoding='utf-8')

    extractor = EnhancedEntityExtractor()

    test_queries = [
        "T√¥i th·ª±c hi·ªán giao d·ªãch chuy·ªÉn ti·ªÅn ƒë·∫øn ng√¢n h√†ng nh∆∞ng t√†i kho·∫£n th·ª• h∆∞·ªüng ch∆∞a nh·∫≠n ƒë∆∞·ª£c th√¨ ph·∫£i l√†m th·∫ø n√†o?",
        "Chuy·ªÉn ti·ªÅn r·ªìi m√† ti·ªÅn ch∆∞a v·ªÅ",
        "Giao d·ªãch n·∫°p ti·ªÅn th·∫•t b·∫°i",
        "L√†m sao ƒë·ªÉ chuy·ªÉn ti·ªÅn ƒë·∫øn Vietinbank?",
        "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c OTP khi chuy·ªÉn ti·ªÅn",
        "T√†i kho·∫£n kh√¥ng h·ª£p l·ªá",
    ]

    print("=" * 80)
    print("ENHANCED ENTITY EXTRACTOR TEST")
    print("=" * 80)

    for query in test_queries:
        print(f"\n{'='*80}")
        print(f"Query: {query}")
        print(f"{'='*80}")

        # Extract with confidence
        entities, confidence = extractor.extract_with_confidence(query)

        print(f"\nüìä Confidence: {confidence:.2%}")

        print("\nüìå Extracted Entities:")
        for entity_type, values in entities.items():
            if values and entity_type != 'out_of_scope':
                print(f"  {entity_type}: {values}")

        # Interpretation
        print(f"\nüí° Interpretation:")
        if confidence >= 0.7:
            print("  ‚úÖ HIGH confidence - Extraction is reliable")
        elif confidence >= 0.4:
            print("  üü° MEDIUM confidence - May need fallback")
        else:
            print("  ‚ö†Ô∏è  LOW confidence - Consider using LLM fallback")

        if entities.get("Error"):
            print("  üîß Detected as TROUBLESHOOTING query")
        elif entities.get("Topic") and not entities.get("Error"):
            print("  üìñ Detected as HOW-TO or INFO query")
