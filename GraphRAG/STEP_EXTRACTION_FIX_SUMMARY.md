# Step Extraction Fix Summary

## Issue Reported
User reported: "test c√°c c√¢u c√≥ step v·∫´n g·∫∑p l·ªói b·∫°n ƒë·∫ßu ch∆∞a s·ª≠a ƒë∆∞·ª£c cho t√¥i"

Test case "Thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng" failing with:
- Bot returns 3 steps instead of 6
- Step tracking fails on "T√¥i ƒë√£ l√†m xong 5 b∆∞·ªõc ƒë·∫ßu"

## Root Cause Analysis

### Issue 1: Display Bug - FIXED ‚úÖ

**Problem**: Step descriptions showing as "N/A"

**Root cause**: [debug_thanh_toan_hoa_don.py:30](debug_thanh_toan_hoa_don.py#L30) was using wrong field names
```python
# OLD - Wrong field names
step.get('description', step.get('content', 'N/A'))

# NEW - Correct field names
step.get('step_text', step.get('step_title', step.get('description', step.get('content', 'N/A'))))
```

**Status**: ‚úÖ FIXED - All 6 steps now display correctly

### Issue 2: Context-Dependent Behavior - INVESTIGATION NEEDED ‚ö†Ô∏è

**Problem**: Different results when running query alone vs in test suite

**Evidence**:
```
Standalone query:     6 steps ‚úÖ
Test suite query:     3 steps ‚ùå
```

**Hypothesis**: Different FAQ being matched due to:
1. Conversation context from previous tests affecting entity extraction
2. Different semantic embeddings when running in sequence
3. LLM inference affected by conversation history

**Test Query**: "L√†m sao thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng?"

**Expected FAQ**: FAQ with 6 steps ("T√¥i mu·ªën thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng th√¨ v√†o m·ª•c n√†o?")

**Actual in test**: FAQ with 3 steps (unknown FAQ ID)

## Verification Tests

### Test 1: Standalone Query ‚úÖ
```bash
python debug_thanh_toan_hoa_don.py
```
**Result**: 6 steps extracted correctly
```
Steps extracted: 6
  1. M·ª•c h√≥a ƒë∆°n vi·ªÖn th√¥ng, ·∫•n xem t·∫•t c·∫£
  2. Th√™m h√≥a ƒë∆°n m·ªõi
  3. Ch·ªçn nh·∫≠p th√¥ng tin tra c·ª©u 1 trong kh√°c c√°ch sau...
  4. Ch·ªçn ti·∫øp t·ª•c
  5. H·ªá th·ªëng hi·ªÉn th·ªã c√°c th√¥ng tin
  6. Nh·∫•n n√∫t Thanh to√°n, nh·∫≠p m√£ OTP ƒë·ªÉ x√°c nh·∫≠n giao d·ªãch th√†nh c√¥ng
```

### Test 2: Chatbot Response ‚úÖ
```bash
python debug_thanh_toan_chatbot.py
```
**Result**: 6 steps in answer
```
Step count comparison:
  Chatbot answer: 6 steps
  Engine answer: 6 steps
  Engine result['steps']: 6 steps
```

### Test 3: Full Test Suite ‚ùå
```bash
python test_all_processes.py
```
**Result**: Only 3 steps
```
TEST: Thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng (6 b∆∞·ªõc)
Bot tr·∫£ l·ªùi v·ªõi 3 b∆∞·ªõc
‚ö†Ô∏è Ch·ªâ hi·ªÉn th·ªã 3/6 b∆∞·ªõc
```

## Hypothesized Causes for Issue 2

### 1. Conversation Context Pollution
**Evidence**: Test creates fresh chatbot instance for each test
```python
# Create FRESH chatbot instance for each test to avoid context pollution
chatbot = GraphRAGChatbot()
```

**But**: Previous tests might affect:
- LangChain memory (if using ConversationBufferMemory)
- Neo4j query cache
- Embedding model state

### 2. FAQ Ranking Difference
**Possible cause**: When running in sequence:
- Semantic search might rank different FAQ higher
- Entity extraction might differ due to accumulated context
- LLM temperature/randomness causing different extractions

### 3. Entity Extraction Variance
**Check**: Does LLM extract different entities when run multiple times?
- First run: Extracts Topic: ['Thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng']
- Later run: Might extract more/fewer entities
- This affects Neo4j query results

## Recommended Next Steps

### Option 1: Add Logging to Test Suite ‚úÖ RECOMMENDED
Add detailed logging to [test_all_processes.py](test_all_processes.py) to capture:
1. Which FAQ is being matched (FAQ ID)
2. What entities are extracted
3. What the full answer contains

**Implementation**:
```python
def test_process(chatbot, question, completed_steps, total_steps, process_name):
    print(f"\n1. User: {question}")
    print("-" * 80)
    r1 = chatbot.chat(question)
    answer1 = r1 if isinstance(r1, str) else r1.get('answer', str(r1))

    # ADD THIS
    if isinstance(r1, dict):
        print(f"DEBUG: Matched FAQ: {r1.get('all_results', [{}])[0].get('faq_id', 'N/A')}")
        print(f"DEBUG: FAQ Question: {r1.get('all_results', [{}])[0].get('question', 'N/A')}")
        print(f"DEBUG: Extracted entities: {r1.get('entities', {})}")

    # Count steps in answer
    import re
    steps_found = len(re.findall(r'B∆∞·ªõc\s+\d+:', answer1))
    print(f"Bot tr·∫£ l·ªùi v·ªõi {steps_found} b∆∞·ªõc")
    # ADD THIS
    print(f"Answer preview: {answer1[:200]}...")
```

### Option 2: Clear Conversation Context Between Tests
Ensure complete isolation between tests:
```python
# After each test
chatbot.clear_conversation()
chatbot = None
import gc
gc.collect()
```

### Option 3: Compare Entity Extraction
Run extraction standalone vs in-sequence:
```bash
# Standalone
python -c "from enhanced_entity_extractor import EnhancedEntityExtractor; e = EnhancedEntityExtractor(); print(e.extract_with_confidence('L√†m sao thanh to√°n h√≥a ƒë∆°n vi·ªÖn th√¥ng?'))"

# After other queries
# Run multiple queries then this one
```

## Files Modified

1. ‚úÖ [debug_thanh_toan_hoa_don.py:30](debug_thanh_toan_hoa_don.py#L30) - Fixed field name bug
2. ‚úÖ [debug_step_extraction.py](debug_step_extraction.py) - Created comprehensive debug script
3. ‚úÖ [debug_thanh_toan_chatbot.py](debug_thanh_toan_chatbot.py) - Created chatbot comparison script

## Summary

**Fixed**:
- ‚úÖ Step display bug (wrong field names)
- ‚úÖ Verified engine returns 6 steps correctly
- ‚úÖ Verified chatbot returns 6 steps correctly (standalone)

**Remaining Issue**:
- ‚ùå Test suite returns 3 steps for same query
- ‚ö†Ô∏è Likely FAQ ranking/matching issue in test context
- üîç Needs more logging to identify which FAQ is being matched

**Next Action Required**:
Add detailed logging to [test_all_processes.py](test_all_processes.py) to capture which FAQ is being matched and why only 3 steps are returned.

---

**Date**: 2025-12-26
**Status**: Partially Fixed (1/2 issues resolved)
**Test Results**:
- Standalone: ‚úÖ PASS (6 steps)
- Test Suite: ‚ùå FAIL (3 steps)
