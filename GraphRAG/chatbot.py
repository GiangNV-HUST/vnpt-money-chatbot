"""
GraphRAG Chatbot for VNPT Money
Combines GraphRAG retrieval with LLM generation
NOW WITH CONVERSATION CONTEXT SUPPORT!
"""

import logging
from typing import Dict, List, Optional

from neo4j_rag_engine import Neo4jGraphRAGEngine
from conversation_context_manager import ConversationContextManager
import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GraphRAGChatbot:
    """Chatbot that uses GraphRAG + LLM for answering questions with conversation context tracking"""

    def __init__(self):
        """Initialize chatbot with GraphRAG engine and LLM"""
        # Initialize GraphRAG engine
        self.rag_engine = Neo4jGraphRAGEngine()

        # Initialize Conversation Context Manager (NEW)
        self.context_manager = ConversationContextManager(max_history=5)
        logger.info("Conversation context manager initialized")

        # Initialize LLM
        self.llm = None
        self._initialize_llm()

        # Conversation history (legacy, now using context_manager)
        self.conversation_history = []

    def _initialize_llm(self):
        """Initialize LLM based on configuration"""
        try:
            if config.LLM_PROVIDER == "openai":
                self._initialize_openai()
            else:
                logger.warning("No LLM configured, using template responses")
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            logger.warning("Falling back to template responses")

    def _initialize_openai(self):
        """Initialize OpenAI LLM"""
        try:
            from openai import OpenAI
            self.llm_client = OpenAI(api_key=config.OPENAI_API_KEY)
            self.llm = "openai"
            logger.info(f"OpenAI LLM initialized: {config.LLM_MODEL}")
        except Exception as e:
            logger.error(f"OpenAI initialization failed: {e}")
            raise

    def chat(self, user_message: str) -> str:
        """
        Process user message and return response WITH CONTEXT AWARENESS

        Args:
            user_message: User's input message

        Returns:
            Chatbot response
        """
        logger.info(f"User: {user_message}")

        # Step 1: Check for contextual references and enhance query
        enhanced_query, continuation_context = self.context_manager.enhance_query_with_context(user_message)

        # Step 2: Retrieve relevant context from GraphRAG
        rag_result = self.rag_engine.query(
            enhanced_query,
            continuation_context=continuation_context
        )

        # Step 3: Generate response
        # CRITICAL FIX: For procedural FAQs with steps, use original answer directly
        # to preserve all steps without LLM summarization
        steps = rag_result.get("steps", [])
        has_steps = steps and len(steps) > 0

        if has_steps and rag_result.get("status") == "success":
            # Procedural FAQ - use original answer to preserve all steps
            logger.info(f"Procedural FAQ detected ({len(steps)} steps), using original answer")
            response = rag_result.get("answer", "")
        elif self.llm:
            # Non-procedural or no steps - use LLM for better formatting
            response = self._generate_llm_response(user_message, rag_result, continuation_context)
        else:
            response = self._generate_template_response(rag_result)

        # Step 4: Add to conversation context
        self.context_manager.add_turn(user_message, rag_result)

        # Step 5: Add to legacy conversation history
        self.conversation_history.append({
            "user": user_message,
            "assistant": response,
            "rag_context": rag_result
        })

        logger.info(f"Assistant: {response}")

        return response

    def _generate_llm_response(self, user_message: str, rag_result: Dict, continuation_context: Optional[Dict] = None) -> str:
        """Generate response using LLM with RAG context"""

        # Build prompt with context
        prompt = self._build_prompt(user_message, rag_result, continuation_context)

        try:
            if self.llm == "openai":
                return self._call_openai(prompt)
            else:
                return self._generate_template_response(rag_result)
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return self._generate_template_response(rag_result)

    def _build_prompt(self, user_message: str, rag_result: Dict, continuation_context: Optional[Dict] = None) -> str:
        """Build prompt for LLM with RAG context"""

        if rag_result["status"] != "success":
            return f"""B·∫°n l√† tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa VNPT Money, t√™n l√† "VNPT Assistant".

C√ÇU H·ªéI: {user_message}

Y√äU C·∫¶U:
R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong c∆° s·ªü d·ªØ li·ªáu v·ªÅ c√¢u h·ªèi n√†y.

H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán v√† l·ªãch s·ª±:
1. Xin l·ªói kh√°ch h√†ng v√¨ ch∆∞a c√≥ th√¥ng tin
2. ƒê·ªÅ xu·∫•t h·ªç li√™n h·ªá Hotline: 1900 8198 ho·∫∑c email: hotro@vnptmoney.vn
3. Gi·ªçng ƒëi·ªáu chuy√™n nghi·ªáp, th·∫•u hi·ªÉu
4. Ng·∫Øn g·ªçn, kh√¥ng d√†i d√≤ng"""

        # Extract context
        answer = rag_result.get("answer", "")
        related_entities = rag_result.get("related_entities", {})
        alternative_actions = rag_result.get("alternative_actions", [])
        related_questions = rag_result.get("related_questions", [])
        steps = rag_result.get("steps", [])
        confidence = rag_result.get("confidence", 0)

        # Build context string with better structure
        context_parts = []

        # Main answer
        context_parts.append(f"üìå TH√îNG TIN CH√çNH:\n{answer}")

        # Steps if available
        if steps and len(steps) > 0:
            steps_text = "\n".join([f"   B∆∞·ªõc {i+1}: {step}" for i, step in enumerate(steps)])
            context_parts.append(f"\nüìù C√ÅC B∆Ø·ªöC TH·ª∞C HI·ªÜN:\n{steps_text}")

        # Related entities
        if related_entities:
            entities_info = []
            if related_entities.get("services"):
                entities_info.append(f"   ‚Ä¢ D·ªãch v·ª•: {', '.join(related_entities['services'])}")
            if related_entities.get("banks"):
                entities_info.append(f"   ‚Ä¢ Ng√¢n h√†ng: {', '.join(related_entities['banks'])}")
            if related_entities.get("errors"):
                entities_info.append(f"   ‚Ä¢ L·ªói li√™n quan: {', '.join(related_entities['errors'])}")
            if related_entities.get("features"):
                entities_info.append(f"   ‚Ä¢ T√≠nh nƒÉng: {', '.join(related_entities['features'])}")

            if entities_info:
                context_parts.append(f"\nüîó TH√îNG TIN LI√äN QUAN:\n" + "\n".join(entities_info))

        # Alternative actions
        if alternative_actions:
            alt_text = "\n".join([f"   ‚Ä¢ {alt['action']}: {alt['reason']}" for alt in alternative_actions])
            context_parts.append(f"\nüí° PH∆Ø∆†NG √ÅN THAY TH·∫æ:\n{alt_text}")

        # Related questions
        if related_questions:
            rq_text = "\n".join([f"   ‚Ä¢ {rq['question']}" for rq in related_questions[:3]])
            context_parts.append(f"\n‚ùì C√ÇU H·ªéI LI√äN QUAN:\n{rq_text}")

        context = "\n".join(context_parts)

        # Add continuation context instructions if present
        # CRITICAL: Skip if answer is a completion message
        continuation_instruction = ""
        is_completion_answer = ("‚úÖ" in answer and ("ho√†n th√†nh t·∫•t c·∫£" in answer or "Hotline: 1900" in answer))

        if continuation_context and not is_completion_answer:
            # Case 1: Status-based continuation
            if continuation_context.get("status_result"):
                status = continuation_context["status_result"]
                next_step = continuation_context.get("next_step", 2)
                continuation_instruction = f"""
üîÑ CONTINUATION CONTEXT (QUAN TR·ªåNG):
- Ng∆∞·ªùi d√πng ƒë√£ ho√†n th√†nh b∆∞·ªõc 1 (ki·ªÉm tra tr·∫°ng th√°i)
- Tr·∫°ng th√°i giao d·ªãch: **{status}**
- B·∫ÆT BU·ªòC: Ch·ªâ tr·∫£ l·ªùi v·ªÅ b∆∞·ªõc {next_step} (tr∆∞·ªùng h·ª£p tr·∫°ng th√°i "{status}")
- KH√îNG l·∫∑p l·∫°i b∆∞·ªõc 1 ho·∫∑c to√†n b·ªô c√°c b∆∞·ªõc
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·∫≠p trung v√†o h√†nh ƒë·ªông c·∫ßn l√†m ti·∫øp theo

"""
            # Case 2: Step-based continuation (e.g., "ƒë√£ ho√†n th√†nh N b∆∞·ªõc ƒë·∫ßu")
            else:
                completed_step = continuation_context.get("completed_step", 1)
                next_step = continuation_context.get("next_step", 2)
                continuation_instruction = f"""
üîÑ CONTINUATION CONTEXT (QUAN TR·ªåNG):
- Ng∆∞·ªùi d√πng ƒë√£ ho√†n th√†nh b∆∞·ªõc 1 ƒë·∫øn b∆∞·ªõc {completed_step}
- ƒêang c·∫ßn h∆∞·ªõng d·∫´n ti·∫øp B·ªòC TI·∫æP THEO (b∆∞·ªõc {next_step})
- B·∫ÆT BU·ªòC: Vi·∫øt c√¢u m·ªü ƒë·∫ßu T·ª∞ NHI√äN nh∆∞ ƒëang ti·∫øp t·ª•c h·ªôi tho·∫°i
- V√ç D·ª§ M·ªû ƒê·∫¶U T·ªêT:
  * "B∆∞·ªõc ti·∫øp theo:"
  * "Ti·∫øp theo, b·∫°n c·∫ßn:"
  * "Sau khi ho√†n th√†nh {completed_step} b∆∞·ªõc ƒë·∫ßu, b·∫°n ti·∫øp t·ª•c:"
- KH√îNG vi·∫øt: "ƒê·ªÉ chuy·ªÉn ti·ªÅn t·ª´ VNPT Money ƒë·∫øn ng√¢n h√†ng:" (qu√° c·ª©ng nh·∫Øc!)
- NG·ªÆ C·∫¢NH ch·ªâ ch·ª©a b∆∞·ªõc {next_step} (KH√îNG ph·∫£i t·∫•t c·∫£ c√°c b∆∞·ªõc c√≤n l·∫°i)
- GI·ªÆ NGUY√äN s·ªë th·ª© t·ª± b∆∞·ªõc trong NG·ªÆ C·∫¢NH, KH√îNG ƒë√°nh s·ªë l·∫°i
- Ch·ªâ c·∫ßn vi·∫øt c√¢u m·ªü ƒë·∫ßu t·ª± nhi√™n r·ªìi sao ch√©p b∆∞·ªõc t·ª´ NG·ªÆ C·∫¢NH

"""

        # Build final prompt with improved instructions
        prompt = f"""B·∫°n l√† VNPT Assistant - tr·ª£ l√Ω ·∫£o c·ªßa VNPT Money.

üéØ NHI·ªÜM V·ª§:
Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH b√™n d∆∞·ªõi.

üìã NGUY√äN T·∫ÆC (QUAN TR·ªåNG):
1. **N·ªôi dung**:
   - CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH, KH√îNG b·ªãa th√™m
   - CH·ªà tr·∫£ l·ªùi ƒê√öNG c√¢u h·ªèi ng∆∞·ªùi d√πng, KH√îNG th√™m th√¥ng tin kh√¥ng li√™n quan
   - N·∫øu NG·ªÆ C·∫¢NH c√≥ nhi·ªÅu FAQ: CH·ªà d√πng FAQ ph√π h·ª£p nh·∫•t v·ªõi c√¢u h·ªèi
2. **Format - NG·∫ÆN G·ªåN**:
   - M·ªñI b∆∞·ªõc XU·ªêNG D√íNG ri√™ng (B∆∞·ªõc 1, B∆∞·ªõc 2,...)
   - KH√îNG d√πng bullet points (‚Ä¢) trong m·ªói b∆∞·ªõc
   - N·ªôi dung m·ªói b∆∞·ªõc vi·∫øt LI·ªÄN M·∫†CH, ng·∫Øn g·ªçn, kh√¥ng xu·ªëng d√≤ng chi ti·∫øt con
3. **Ph·∫ßn "L∆∞u √Ω"**: CH·ªà bao g·ªìm n·∫øu n√≥ TR·ª∞C TI·∫æP li√™n quan ƒë·∫øn c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi
4. **Icon/Emoji**: C√ì TH·ªÇ th√™m icon th√¢n thi·ªán (‚ö†Ô∏è üí° ‚úÖ ‚ùå üìû) khi ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t th√¥ng tin quan tr·ªçng
5. **KH√îNG th√™m**: C√¢u m·ªü ƒë·∫ßu d√†i "Ch√†o b·∫°n! T√¥i hi·ªÉu...", "C√¢u h·ªèi li√™n quan" kh√¥ng c·∫ßn thi·∫øt, ho·∫∑c "L∆∞u √Ω" t·ª´ FAQ kh√°c
6. **‚ö†Ô∏è COMPLETION MESSAGE**: N·∫øu NG·ªÆ C·∫¢NH ch·ª©a th√¥ng b√°o ho√†n th√†nh (c√≥ ‚úÖ, "ƒë√£ ho√†n th√†nh t·∫•t c·∫£", "Hotline: 1900"), GI·ªÆ NGUY√äN th√¥ng b√°o ƒë√≥, KH√îNG ƒë·ªïi th√†nh format b∆∞·ªõc

üìã V√ç D·ª§ FORMAT T·ªêT:

**C√¢u h·ªèi ƒë·∫ßu ti√™n** (ch∆∞a c√≥ context):
```
ƒê·ªÉ chuy·ªÉn ti·ªÅn t·ª´ VNPT Money ƒë·∫øn ng√¢n h√†ng:

B∆∞·ªõc 1: Ch·ªçn chuy·ªÉn "ƒê·∫øn ng√¢n h√†ng", nh·∫•n v√†o t√πy ch·ªçn "ƒê·∫øn ng√¢n h√†ng"
B∆∞·ªõc 2: Ch·ªçn "Qua s·ªë t√†i kho·∫£n/s·ªë th·∫ª", ch·ªçn ph∆∞∆°ng th·ª©c chuy·ªÉn
B∆∞·ªõc 3: Ch·ªçn ng√¢n h√†ng c·∫ßn chuy·ªÉn, l·ª±a ch·ªçn ng√¢n h√†ng m√† b·∫°n mu·ªën chuy·ªÉn ti·ªÅn ƒë·∫øn

‚ö†Ô∏è L∆∞u √Ω: Ngay sau khi b·∫°n ho√†n t·∫•t giao d·ªãch chuy·ªÉn ti·ªÅn, ng∆∞·ªùi nh·∫≠n s·∫Ω nh·∫≠n ƒë∆∞·ª£c ti·ªÅn trong t√†i kho·∫£n ng√¢n h√†ng.
```

**C√¢u h·ªèi ti·∫øp theo** (ƒë√£ ho√†n th√†nh 3 b∆∞·ªõc ƒë·∫ßu, c·∫ßn h∆∞·ªõng d·∫´n b∆∞·ªõc 4):
```
B∆∞·ªõc ti·∫øp theo:

B∆∞·ªõc 4: Nh·∫≠p s·ªë t√†i kho·∫£n/s·ªë th·∫ª v√† ·∫•n Ki·ªÉm tra
```
(L∆∞u √Ω: C√¢u m·ªü ƒë·∫ßu ng·∫Øn g·ªçn "B∆∞·ªõc ti·∫øp theo:", GI·ªÆ NGUY√äN s·ªë b∆∞·ªõc 4, KH√îNG ƒë√°nh l·∫°i th√†nh "B∆∞·ªõc 1")

**Completion message** (ƒë√£ ho√†n th√†nh T·∫§T C·∫¢ c√°c b∆∞·ªõc):
```
‚úÖ B·∫°n ƒë√£ ho√†n th√†nh t·∫•t c·∫£ 5 b∆∞·ªõc!

N·∫øu b·∫°n v·∫´n g·∫∑p v·∫•n ƒë·ªÅ ho·∫∑c c·∫ßn h·ªó tr·ª£ th√™m, vui l√≤ng li√™n h·ªá:
üìû Hotline: 1900 8198 (24/7)
‚úâÔ∏è Email: hotro@vnptmoney.vn
```
(L∆∞u √Ω: GI·ªÆ NGUY√äN to√†n b·ªô completion message, KH√îNG format l·∫°i)

‚ö†Ô∏è CRITICAL:
- M·ªói b∆∞·ªõc PH·∫¢I xu·ªëng d√≤ng ri√™ng
- KH√îNG d√πng bullet points (‚Ä¢), n·ªôi dung trong b∆∞·ªõc vi·∫øt li·ªÅn
- CH·ªà bao g·ªìm "L∆∞u √Ω" n·∫øu n√≥ TR·ª∞C TI·∫æP li√™n quan ƒë·∫øn c√¢u h·ªèi (KH√îNG t·ª± ƒë·ªông th√™m t·ª´ FAQ kh√°c!)
- N·∫øu l√† COMPLETION MESSAGE: GI·ªÆ NGUY√äN, kh√¥ng format l·∫°i

{continuation_instruction}üìö NG·ªÆ C·∫¢NH (ƒê·ªô tin c·∫≠y: {confidence:.0%}):
{context}

‚ùì C√ÇU H·ªéI:
"{user_message}"

üí¨ TR·∫¢ L·ªúI (format d·ªÖ ƒë·ªçc nh∆∞ v√≠ d·ª• - m·ªói b∆∞·ªõc/bullet xu·ªëng d√≤ng):
"""

        return prompt

    def _call_openai(self, prompt: str) -> str:
        """Call OpenAI API"""
        try:
            # System message with formatting instructions
            system_message = """B·∫°n l√† VNPT Assistant - tr·ª£ l√Ω ·∫£o c·ªßa VNPT Money.

NHI·ªÜM V·ª§ CH√çNH:
- Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH
- Format NG·∫ÆN G·ªåN: KH√îNG xu·ªëng d√≤ng nhi·ªÅu, KH√îNG d√πng bullet points (‚Ä¢)
- KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin

QUY T·∫ÆC FORMAT (CRITICAL):
- M·ªñI b∆∞·ªõc XU·ªêNG D√íNG ri√™ng
- KH√îNG d√πng bullet points (‚Ä¢)
- N·ªôi dung trong b∆∞·ªõc vi·∫øt LI·ªÄN M·∫†CH, ng·∫Øn g·ªçn, kh√¥ng xu·ªëng d√≤ng chi ti·∫øt con
- CH·ªà bao g·ªìm "L∆∞u √Ω" n·∫øu n√≥ TR·ª∞C TI·∫æP li√™n quan ƒë·∫øn c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi
- C√ì TH·ªÇ th√™m icon th√¢n thi·ªán (‚ö†Ô∏è üí° ‚úÖ ‚ùå üìû) khi ph√π h·ª£p
- KH√îNG th√™m: "Ch√†o b·∫°n", "C√¢u h·ªèi li√™n quan", ho·∫∑c "L∆∞u √Ω" t·ª´ FAQ kh√¥ng li√™n quan
- KHI TI·∫æP T·ª§C H·ªòI THO·∫†I: D√πng c√¢u m·ªü ƒë·∫ßu t·ª± nhi√™n nh∆∞ "C√°c b∆∞·ªõc ti·∫øp theo l√†:", "Ti·∫øp theo, b·∫°n c·∫ßn l√†m:", KH√îNG l·∫∑p l·∫°i intro ban ƒë·∫ßu
- ‚ö†Ô∏è COMPLETION MESSAGE: N·∫øu NG·ªÆ C·∫¢NH c√≥ th√¥ng b√°o ho√†n th√†nh (‚úÖ, "ƒë√£ ho√†n th√†nh t·∫•t c·∫£", "Hotline: 1900"), GI·ªÆ NGUY√äN to√†n b·ªô, KH√îNG format l·∫°i

V√ç D·ª§ FORMAT:
1. C√¢u h·ªèi ƒë·∫ßu (8 b∆∞·ªõc): "ƒê·ªÉ n·∫°p ti·ªÅn: B∆∞·ªõc 1: ..., B∆∞·ªõc 2: ..., B∆∞·ªõc 3: ..."
2. C√¢u ti·∫øp theo (ƒë√£ l√†m 3 b∆∞·ªõc, c·∫ßn b∆∞·ªõc 4): "B∆∞·ªõc ti·∫øp theo: B∆∞·ªõc 4: ..." (GI·ªÆ NGUY√äN s·ªë 4, KH√îNG ƒë√°nh l·∫°i th√†nh B∆∞·ªõc 1!)
3. Ho√†n th√†nh t·∫•t c·∫£: "‚úÖ B·∫°n ƒë√£ ho√†n th√†nh t·∫•t c·∫£ X b∆∞·ªõc! N·∫øu b·∫°n v·∫´n g·∫∑p v·∫•n ƒë·ªÅ... üìû Hotline: 1900 8198" (GI·ªÆ NGUY√äN, KH√îNG ƒë·ªïi format)
"""

            # Call OpenAI API
            response = self.llm_client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=config.LLM_TEMPERATURE,
                max_tokens=config.LLM_MAX_TOKENS
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise

    def _generate_template_response(self, rag_result: Dict) -> str:
        """Generate template response without LLM"""

        if rag_result["status"] != "success":
            return """Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n trong c∆° s·ªü d·ªØ li·ªáu.

ƒê·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·ªët h∆°n, b·∫°n c√≥ th·ªÉ:
üìû G·ªçi Hotline: 1900 8198 (24/7)
‚úâÔ∏è Email: hotro@vnptmoney.vn
üåê Truy c·∫≠p: https://vnptpay.vn

Ch√∫ng t√¥i lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!"""

        # Extract information
        answer = rag_result.get("answer", "")
        alternative_actions = rag_result.get("alternative_actions", [])
        related_questions = rag_result.get("related_questions", [])
        related_entities = rag_result.get("related_entities", {})
        confidence = rag_result.get("confidence", 0)

        # Build response
        response_parts = []

        # Main answer with confidence indicator
        if confidence >= 0.8:
            response_parts.append(f"{answer}")
        else:
            response_parts.append(f"D·ª±a tr√™n th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c:\n\n{answer}")

        # Alternative actions
        if alternative_actions:
            response_parts.append("\nüí° **Ph∆∞∆°ng √°n thay th·∫ø:**")
            for i, alt in enumerate(alternative_actions, 1):
                response_parts.append(f"{i}. {alt['action']}: {alt['reason']}")

        # Additional info from entities
        if related_entities:
            notes = []
            if related_entities.get("errors"):
                notes.append(f"‚ö†Ô∏è L·ªói li√™n quan: {', '.join(related_entities['errors'])}")
            if related_entities.get("features"):
                notes.append(f"‚ú® T√≠nh nƒÉng: {', '.join(related_entities['features'])}")

            if notes:
                response_parts.append("\n" + "\n".join(notes))

        # Related questions
        if related_questions:
            response_parts.append("\n‚ùì **B·∫°n c√≥ th·ªÉ quan t√¢m:**")
            for i, rq in enumerate(related_questions[:3], 1):
                response_parts.append(f"{i}. {rq['question']}")

        # Footer
        response_parts.append("\n---")
        response_parts.append("N·∫øu c·∫ßn h·ªó tr·ª£ th√™m, vui l√≤ng li√™n h·ªá Hotline: 1900 8198")

        return "\n".join(response_parts)

    def get_conversation_history(self) -> List[Dict]:
        """Get conversation history"""
        return self.conversation_history

    def clear_history(self):
        """Clear conversation history and context"""
        self.conversation_history = []
        self.context_manager.clear_context()
        logger.info("Conversation history and context cleared")

    def get_chat_statistics(self) -> Dict:
        """Get chatbot statistics with context info"""
        context_summary = self.context_manager.get_summary()
        return {
            "total_conversations": len(self.conversation_history),
            "llm_enabled": self.llm is not None,
            "llm_provider": config.LLM_PROVIDER,
            "cache_size": len(self.rag_engine.cache),
            "context_turns": context_summary.get("num_turns", 0),
            "current_topic": context_summary.get("current_topic"),
            "has_active_context": context_summary.get("has_active_context", False)
        }


# ============================================
# INTERACTIVE CHAT
# ============================================

def interactive_chat():
    """Run interactive chatbot session"""
    print("=" * 60)
    print("VNPT Money GraphRAG Chatbot")
    print("=" * 60)
    print("G√µ 'exit' ho·∫∑c 'quit' ƒë·ªÉ tho√°t")
    print("G√µ 'clear' ƒë·ªÉ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i")
    print("G√µ 'stats' ƒë·ªÉ xem th·ªëng k√™")
    print("=" * 60)

    chatbot = GraphRAGChatbot()

    while True:
        try:
            user_input = input("\n\nB·∫°n: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['exit', 'quit', 'tho√°t']:
                print("\nC·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng VNPT Money Chatbot. H·∫πn g·∫∑p l·∫°i!")
                break

            if user_input.lower() == 'clear':
                chatbot.clear_history()
                print("\nƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i.")
                continue

            if user_input.lower() == 'stats':
                stats = chatbot.get_chat_statistics()
                print(f"\nTh·ªëng k√™:")
                print(f"  - S·ªë l∆∞·ª£t h·ªôi tho·∫°i: {stats['total_conversations']}")
                print(f"  - LLM: {stats['llm_provider']} ({'Enabled' if stats['llm_enabled'] else 'Disabled'})")
                print(f"  - Cache size: {stats['cache_size']}")
                continue

            # Get response
            response = chatbot.chat(user_input)

            print(f"\nVNPT Money Bot: {response}")

        except KeyboardInterrupt:
            print("\n\nC·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng VNPT Money Chatbot. H·∫πn g·∫∑p l·∫°i!")
            break
        except Exception as e:
            logger.error(f"Error in chat loop: {e}")
            print(f"\nXin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i.")


if __name__ == "__main__":
    interactive_chat()
