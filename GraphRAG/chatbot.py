"""
GraphRAG Chatbot for VNPT Money
Combines GraphRAG retrieval with LLM generation
NOW WITH CONVERSATION CONTEXT SUPPORT!
"""

import logging
from typing import Dict, List, Optional

from neo4j_rag_engine import Neo4jGraphRAGEngine
from conversation_context_manager import ConversationContextManager
import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GraphRAGChatbot:
    """Chatbot that uses GraphRAG + LLM for answering questions with conversation context tracking"""

    def __init__(self):
        """Initialize chatbot with GraphRAG engine and LLM"""
        # Initialize GraphRAG engine
        self.rag_engine = Neo4jGraphRAGEngine()

        # Initialize Conversation Context Manager (NEW)
        self.context_manager = ConversationContextManager(max_history=5)
        logger.info("Conversation context manager initialized")

        # Initialize LLM
        self.llm = None
        self._initialize_llm()

        # Conversation history (legacy, now using context_manager)
        self.conversation_history = []

    def _initialize_llm(self):
        """Initialize LLM based on configuration"""
        try:
            if config.LLM_PROVIDER == "openai":
                self._initialize_openai()
            else:
                logger.warning("No LLM configured, using template responses")
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            logger.warning("Falling back to template responses")

    def _initialize_openai(self):
        """Initialize OpenAI LLM"""
        try:
            from openai import OpenAI
            self.llm_client = OpenAI(api_key=config.OPENAI_API_KEY)
            self.llm = "openai"
            logger.info(f"OpenAI LLM initialized: {config.LLM_MODEL}")
        except Exception as e:
            logger.error(f"OpenAI initialization failed: {e}")
            raise

    def _is_chitchat(self, message: str) -> bool:
        """Check if message is chitchat/greeting (not FAQ-related)"""
        import re

        message_lower = message.lower().strip()

        # CRITICAL FIX: Use word boundaries to prevent false positives
        # Example: "t√™n b·∫°n l√† g√¨?" should match, but "H·ªç t√™n b·∫°n kh√¥ng tr√πng" should NOT

        # Greeting patterns (use word boundaries)
        greeting_patterns = [
            r'\bxin ch√†o\b', r'\bch√†o b·∫°n\b', r'\bhello\b', r'\bhi\b',
            r'^ch√†o\b', r'\bch√†o$'  # "ch√†o" at start or end only
        ]

        # Identity question patterns (more specific)
        identity_patterns = [
            r'\bb·∫°n l√† ai\b', r'\bb·∫°n l√† g√¨\b',
            r'\bt√™n b·∫°n l√†\b', r'\bt√™n b·∫°n\?',  # "t√™n b·∫°n l√† g√¨?" or "t√™n b·∫°n?"
            r'^t√™n b·∫°n\b', # Only at start of message
            r'\bai ƒë√¢y\b'
        ]

        # Thanks patterns
        thanks_patterns = [
            r'\bc·∫£m ∆°n\b', r'\bc√°m ∆°n\b', r'\bthank\b', r'\bthanks\b'
        ]

        # Goodbye patterns
        goodbye_patterns = [
            r'\bt·∫°m bi·ªát\b', r'\bbye\b', r'\bgoodbye\b'
        ]

        # Capability question patterns
        capability_patterns = [
            r'\bb·∫°n l√†m ƒë∆∞·ª£c g√¨\b', r'\bb·∫°n c√≥ th·ªÉ l√†m g√¨\b', r'\bgi√∫p g√¨ ƒë∆∞·ª£c\b'
        ]

        # Combine all patterns
        all_patterns = (greeting_patterns + identity_patterns + thanks_patterns +
                       goodbye_patterns + capability_patterns)

        # Check if any pattern matches
        for pattern in all_patterns:
            if re.search(pattern, message_lower):
                return True

        return False

    def _handle_chitchat(self, message: str) -> str:
        """Handle chitchat/greeting messages"""
        message_lower = message.lower().strip()

        # Greetings
        if any(x in message_lower for x in ["xin ch√†o", "ch√†o b·∫°n", "hello", "hi ", "ch√†o"]):
            return "Xin ch√†o! T√¥i l√† VNPT Assistant, tr·ª£ l√Ω ·∫£o c·ªßa VNPT Money. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n gi·∫£i ƒë√°p c√°c th·∫Øc m·∫Øc v·ªÅ d·ªãch v·ª• VNPT Money. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"

        # Identity
        if any(x in message_lower for x in ["b·∫°n l√† ai", "b·∫°n l√† g√¨", "t√™n b·∫°n", "ai ƒë√¢y"]):
            return "T√¥i l√† VNPT Assistant - tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa VNPT Money. T√¥i ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi√∫p b·∫°n gi·∫£i ƒë√°p c√°c c√¢u h·ªèi v·ªÅ d·ªãch v·ª• v√≠ ƒëi·ªán t·ª≠ VNPT Money, bao g·ªìm: n·∫°p ti·ªÅn, r√∫t ti·ªÅn, chuy·ªÉn ti·ªÅn, li√™n k·∫øt ng√¢n h√†ng v√† c√°c t√≠nh nƒÉng kh√°c. B·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ VNPT Money kh√¥ng?"

        # Thanks
        if any(x in message_lower for x in ["c·∫£m ∆°n", "c√°m ∆°n", "thank"]):
            return "R·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n! N·∫øu c√≥ th√™m c√¢u h·ªèi, ƒë·ª´ng ng·∫°i h·ªèi t√¥i nh√©!"

        # Goodbyes
        if any(x in message_lower for x in ["t·∫°m bi·ªát", "bye", "goodbye"]):
            return "T·∫°m bi·ªát! Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh. H·∫πn g·∫∑p l·∫°i!"

        # Capability
        if any(x in message_lower for x in ["b·∫°n l√†m ƒë∆∞·ª£c g√¨", "b·∫°n c√≥ th·ªÉ l√†m g√¨", "gi√∫p g√¨ ƒë∆∞·ª£c"]):
            return "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n:\n- H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng c√°c t√≠nh nƒÉng VNPT Money (n·∫°p ti·ªÅn, r√∫t ti·ªÅn, chuy·ªÉn ti·ªÅn, thanh to√°n h√≥a ƒë∆°n...)\n- Gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t (l·ªói giao d·ªãch, li√™n k·∫øt ng√¢n h√†ng, ƒë·ªãnh danh...)\n- T∆∞ v·∫•n v·ªÅ ph√≠, h·∫°n m·ª©c, ƒëi·ªÅu ki·ªán s·ª≠ d·ª•ng\n- V√† nhi·ªÅu th√¥ng tin kh√°c v·ªÅ VNPT Money!\n\nB·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"

        # Default chitchat
        return "T√¥i l√† VNPT Assistant. B·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ d·ªãch v·ª• VNPT Money kh√¥ng?"

    def chat(self, user_message: str) -> str:
        """
        Process user message and return response WITH CONTEXT AWARENESS

        Args:
            user_message: User's input message

        Returns:
            Chatbot response
        """
        logger.info(f"User: {user_message}")

        # Step 0: Handle chitchat/greetings first (NEW)
        if self._is_chitchat(user_message):
            response = self._handle_chitchat(user_message)
            logger.info(f"Assistant (chitchat): {response}")
            return response

        # Step 1: Check for contextual references and enhance query
        enhanced_query, continuation_context = self.context_manager.enhance_query_with_context(user_message)

        # Step 2: Retrieve relevant context from GraphRAG
        rag_result = self.rag_engine.query(
            enhanced_query,
            continuation_context=continuation_context
        )

        # Step 3: Generate response
        # CRITICAL FIX: For procedural FAQs with steps, use original answer directly
        # to preserve all steps without LLM summarization
        steps = rag_result.get("steps", [])
        has_steps = steps and len(steps) > 0

        if has_steps and rag_result.get("status") == "success":
            # Procedural FAQ - use original answer to preserve all steps
            logger.info(f"Procedural FAQ detected ({len(steps)} steps), using original answer")
            response = rag_result.get("answer", "")
            # Apply formatting post-processor to improve readability
            response = self._format_answer_for_readability(response)
        elif self.llm:
            # Non-procedural or no steps - use LLM for better formatting
            response = self._generate_llm_response(user_message, rag_result, continuation_context)
        else:
            response = self._generate_template_response(rag_result)

        # Step 4: Add to conversation context
        self.context_manager.add_turn(user_message, rag_result)

        # Step 5: Add to legacy conversation history
        self.conversation_history.append({
            "user": user_message,
            "assistant": response,
            "rag_context": rag_result
        })

        logger.info(f"Assistant: {response}")

        return response

    def _generate_llm_response(self, user_message: str, rag_result: Dict, continuation_context: Optional[Dict] = None) -> str:
        """Generate response using LLM with RAG context"""

        # Build prompt with context
        prompt = self._build_prompt(user_message, rag_result, continuation_context)

        try:
            if self.llm == "openai":
                return self._call_openai(prompt)
            else:
                return self._generate_template_response(rag_result)
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return self._generate_template_response(rag_result)

    def _build_prompt(self, user_message: str, rag_result: Dict, continuation_context: Optional[Dict] = None) -> str:
        """Build prompt for LLM with RAG context"""

        if rag_result["status"] != "success":
            return f"""B·∫°n l√† tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa VNPT Money, t√™n l√† "VNPT Assistant".

C√ÇU H·ªéI: {user_message}

Y√äU C·∫¶U:
R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong c∆° s·ªü d·ªØ li·ªáu v·ªÅ c√¢u h·ªèi n√†y.

H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán v√† l·ªãch s·ª±:
1. Xin l·ªói kh√°ch h√†ng v√¨ ch∆∞a c√≥ th√¥ng tin
2. ƒê·ªÅ xu·∫•t h·ªç li√™n h·ªá Hotline: 1900 8198 ho·∫∑c email: hotro@vnptmoney.vn
3. Gi·ªçng ƒëi·ªáu chuy√™n nghi·ªáp, th·∫•u hi·ªÉu
4. Ng·∫Øn g·ªçn, kh√¥ng d√†i d√≤ng"""

        # Extract context
        answer = rag_result.get("answer", "")
        related_entities = rag_result.get("related_entities", {})
        alternative_actions = rag_result.get("alternative_actions", [])
        related_questions = rag_result.get("related_questions", [])
        steps = rag_result.get("steps", [])
        confidence = rag_result.get("confidence", 0)

        # Build context string with better structure
        context_parts = []

        # Main answer
        context_parts.append(f"üìå TH√îNG TIN CH√çNH:\n{answer}")

        # Steps if available
        if steps and len(steps) > 0:
            steps_text = "\n".join([f"   B∆∞·ªõc {i+1}: {step}" for i, step in enumerate(steps)])
            context_parts.append(f"\nüìù C√ÅC B∆Ø·ªöC TH·ª∞C HI·ªÜN:\n{steps_text}")

        # Related entities
        if related_entities:
            entities_info = []
            if related_entities.get("services"):
                entities_info.append(f"   ‚Ä¢ D·ªãch v·ª•: {', '.join(related_entities['services'])}")
            if related_entities.get("banks"):
                entities_info.append(f"   ‚Ä¢ Ng√¢n h√†ng: {', '.join(related_entities['banks'])}")
            if related_entities.get("errors"):
                entities_info.append(f"   ‚Ä¢ L·ªói li√™n quan: {', '.join(related_entities['errors'])}")
            if related_entities.get("features"):
                entities_info.append(f"   ‚Ä¢ T√≠nh nƒÉng: {', '.join(related_entities['features'])}")

            if entities_info:
                context_parts.append(f"\nüîó TH√îNG TIN LI√äN QUAN:\n" + "\n".join(entities_info))

        # Alternative actions
        if alternative_actions:
            alt_text = "\n".join([f"   ‚Ä¢ {alt['action']}: {alt['reason']}" for alt in alternative_actions])
            context_parts.append(f"\nüí° PH∆Ø∆†NG √ÅN THAY TH·∫æ:\n{alt_text}")

        # Related questions
        if related_questions:
            rq_text = "\n".join([f"   ‚Ä¢ {rq['question']}" for rq in related_questions[:3]])
            context_parts.append(f"\n‚ùì C√ÇU H·ªéI LI√äN QUAN:\n{rq_text}")

        context = "\n".join(context_parts)

        # Add continuation context instructions if present
        # CRITICAL: Skip if answer is a completion message
        continuation_instruction = ""
        is_completion_answer = ("‚úÖ" in answer and ("ho√†n th√†nh t·∫•t c·∫£" in answer or "Hotline: 1900" in answer))

        if continuation_context and not is_completion_answer:
            # Case 1: Status-based continuation
            if continuation_context.get("status_result"):
                status = continuation_context["status_result"]
                next_step = continuation_context.get("next_step", 2)
                continuation_instruction = f"""
üîÑ CONTINUATION CONTEXT (QUAN TR·ªåNG):
- Ng∆∞·ªùi d√πng ƒë√£ ho√†n th√†nh b∆∞·ªõc 1 (ki·ªÉm tra tr·∫°ng th√°i)
- Tr·∫°ng th√°i giao d·ªãch: **{status}**
- B·∫ÆT BU·ªòC: Ch·ªâ tr·∫£ l·ªùi v·ªÅ b∆∞·ªõc {next_step} (tr∆∞·ªùng h·ª£p tr·∫°ng th√°i "{status}")
- KH√îNG l·∫∑p l·∫°i b∆∞·ªõc 1 ho·∫∑c to√†n b·ªô c√°c b∆∞·ªõc
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·∫≠p trung v√†o h√†nh ƒë·ªông c·∫ßn l√†m ti·∫øp theo

"""
            # Case 2: Step-based continuation (e.g., "ƒë√£ ho√†n th√†nh N b∆∞·ªõc ƒë·∫ßu")
            else:
                completed_step = continuation_context.get("completed_step", 1)
                next_step = continuation_context.get("next_step", 2)
                continuation_instruction = f"""
üîÑ CONTINUATION CONTEXT (QUAN TR·ªåNG):
- Ng∆∞·ªùi d√πng ƒë√£ ho√†n th√†nh b∆∞·ªõc 1 ƒë·∫øn b∆∞·ªõc {completed_step}
- ƒêang c·∫ßn h∆∞·ªõng d·∫´n ti·∫øp B·ªòC TI·∫æP THEO (b∆∞·ªõc {next_step})
- B·∫ÆT BU·ªòC: Vi·∫øt c√¢u m·ªü ƒë·∫ßu T·ª∞ NHI√äN nh∆∞ ƒëang ti·∫øp t·ª•c h·ªôi tho·∫°i
- V√ç D·ª§ M·ªû ƒê·∫¶U T·ªêT:
  * "B∆∞·ªõc ti·∫øp theo:"
  * "Ti·∫øp theo, b·∫°n c·∫ßn:"
  * "Sau khi ho√†n th√†nh {completed_step} b∆∞·ªõc ƒë·∫ßu, b·∫°n ti·∫øp t·ª•c:"
- KH√îNG vi·∫øt: "ƒê·ªÉ chuy·ªÉn ti·ªÅn t·ª´ VNPT Money ƒë·∫øn ng√¢n h√†ng:" (qu√° c·ª©ng nh·∫Øc!)
- NG·ªÆ C·∫¢NH ch·ªâ ch·ª©a b∆∞·ªõc {next_step} (KH√îNG ph·∫£i t·∫•t c·∫£ c√°c b∆∞·ªõc c√≤n l·∫°i)
- GI·ªÆ NGUY√äN s·ªë th·ª© t·ª± b∆∞·ªõc trong NG·ªÆ C·∫¢NH, KH√îNG ƒë√°nh s·ªë l·∫°i
- Ch·ªâ c·∫ßn vi·∫øt c√¢u m·ªü ƒë·∫ßu t·ª± nhi√™n r·ªìi sao ch√©p b∆∞·ªõc t·ª´ NG·ªÆ C·∫¢NH

"""

        # Build final prompt with improved instructions
        prompt = f"""B·∫°n l√† VNPT Assistant - tr·ª£ l√Ω ·∫£o c·ªßa VNPT Money, n√≥i chuy·ªán T·ª∞ NHI√äN nh∆∞ m·ªôt ng∆∞·ªùi t∆∞ v·∫•n vi√™n th√¢n thi·ªán, KH√îNG ph·∫£i l√† bot.

üéØ NHI·ªÜM V·ª§:
Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH, nh∆∞ng ph·∫£i VI·∫æT L·∫†I theo phong c√°ch T·ª∞ NHI√äN, TH√ÇN THI·ªÜN, D·ªÑ HI·ªÇU nh∆∞ ƒëang t∆∞ v·∫•n 1-1 cho kh√°ch h√†ng.

üìã NGUY√äN T·∫ÆC (QUAN TR·ªåNG):
1. **N·ªôi dung**:
   - CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH, KH√îNG b·ªãa th√™m
   - CH·ªà tr·∫£ l·ªùi ƒê√öNG c√¢u h·ªèi ng∆∞·ªùi d√πng, KH√îNG th√™m th√¥ng tin kh√¥ng li√™n quan
   - N·∫øu NG·ªÆ C·∫¢NH c√≥ nhi·ªÅu FAQ: CH·ªà d√πng FAQ ph√π h·ª£p nh·∫•t v·ªõi c√¢u h·ªèi
   - ‚ö†Ô∏è **QUAN TR·ªåNG - L·ªåC PH∆Ø∆†NG TH·ª®C C·ª§ TH·ªÇ**:
     * N·∫øu c√¢u h·ªèi ƒë·ªÅ c·∫≠p ƒë·∫øn PH∆Ø∆†NG TH·ª®C/T√çNH NƒÇNG C·ª§ TH·ªÇ (v√≠ d·ª•: "ng√¢n h√†ng li√™n k·∫øt", "QR code", "chuy·ªÉn kho·∫£n", "v√≠ ƒëi·ªán t·ª≠")
     * V√Ä FAQ ch·ª©a NHI·ªÄU ph∆∞∆°ng th·ª©c/h√¨nh th·ª©c kh√°c nhau
     * Th√¨ CH·ªà tr√≠ch xu·∫•t v√† tr·∫£ l·ªùi v·ªÅ ph∆∞∆°ng th·ª©c ƒë∆∞·ª£c h·ªèi, B·ªé QUA c√°c ph∆∞∆°ng th·ª©c kh√°c
     * V√ç D·ª§: N·∫øu h·ªèi "n·∫°p ti·ªÅn t·ª´ ng√¢n h√†ng li√™n k·∫øt" ‚Üí CH·ªà tr·∫£ l·ªùi v·ªÅ ph∆∞∆°ng th·ª©c li√™n k·∫øt, KH√îNG k·ªÉ "n·∫°p b·∫±ng QR" ho·∫∑c ph∆∞∆°ng th·ª©c kh√°c

2. **Gi·ªçng ƒëi·ªáu - T·ª∞ NHI√äN NH∆Ø NG∆Ø·ªúI TH·∫¨T (‚ö†Ô∏è QUAN TR·ªåNG NH·∫§T)**:
   - **TR√ÅNH ng√¥n ng·ªØ c·ª©ng nh·∫Øc ki·ªÉu bot**: KH√îNG d√πng "B∆∞·ªõc 1, B∆∞·ªõc 2, B∆∞·ªõc 3" TR·ª™ KHI ng·ªØ c·∫£nh G·ªêC c√≥ s·∫µn
   - **S·ª¨ D·ª§NG chuy·ªÉn ti·∫øp t·ª± nhi√™n**: "ƒê·∫ßu ti√™n...", "Ti·∫øp theo...", "Sau ƒë√≥...", "Cu·ªëi c√πng..."
   - **TH√äM ƒë·ªông vi√™n v√† c·∫£m x√∫c**: "ƒë·ª´ng lo nh√©", "r·∫•t ƒë∆°n gi·∫£n", "d·ªÖ d√†ng th√¥i", "ch·ªâ c·∫ßn..."
   - **D√ôNG ng√¥n ng·ªØ th√¢n m·∫≠t**: "b·∫°n", "m√¨nh", "nh√©", "nha"
   - **N√ìI nh∆∞ ƒëang t∆∞ v·∫•n tr·ª±c ti·∫øp**: M∆∞·ª£t m√†, th·∫•u hi·ªÉu, kh√¥ng c·ª©ng nh·∫Øc

   - **N·∫æU NG·ªÆ C·∫¢NH c√≥ "B∆∞·ªõc 1, 2, 3"** ‚Üí VI·∫æT L·∫†I t·ª± nhi√™n h∆°n:
     * THAY V√å: "B∆∞·ªõc 1: Ch·ªçn..., B∆∞·ªõc 2: Nh·∫≠p..., B∆∞·ªõc 3: X√°c nh·∫≠n..."
     * VI·∫æT TH√ÄNH: "ƒê·∫ßu ti√™n, b·∫°n c·∫ßn ch·ªçn... Ti·∫øp theo, b·∫°n nh·∫≠p... Sau ƒë√≥ x√°c nh·∫≠n... Cu·ªëi c√πng..."

   - **N·∫æU NG·ªÆ C·∫¢NH KH√îNG c√≥ "B∆∞·ªõc"** ‚Üí Tr·∫£ l·ªùi th√¥ng th∆∞·ªùng, t·ª± nhi√™n
     * Gi·∫£i th√≠ch ng·∫Øn g·ªçn, th√¢n thi·ªán
     * KH√îNG t·ª± √Ω th√™m "B∆∞·ªõc 1, 2, 3"

3. **Ph·∫ßn "L∆∞u √Ω"**: CH·ªà bao g·ªìm n·∫øu n√≥ TR·ª∞C TI·∫æP li√™n quan ƒë·∫øn c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi
4. **Icon/Emoji**: C√ì TH·ªÇ th√™m icon th√¢n thi·ªán (‚ö†Ô∏è üí° ‚úÖ ‚ùå üìû) khi ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t th√¥ng tin quan tr·ªçng
5. **KH√îNG th√™m**: C√¢u m·ªü ƒë·∫ßu d√†i "Ch√†o b·∫°n! T√¥i hi·ªÉu...", "C√¢u h·ªèi li√™n quan" kh√¥ng c·∫ßn thi·∫øt, ho·∫∑c "L∆∞u √Ω" t·ª´ FAQ kh√°c
6. **‚ö†Ô∏è COMPLETION MESSAGE**: N·∫øu NG·ªÆ C·∫¢NH ch·ª©a th√¥ng b√°o ho√†n th√†nh (c√≥ ‚úÖ, "ƒë√£ ho√†n th√†nh t·∫•t c·∫£", "Hotline: 1900"), GI·ªÆ NGUY√äN th√¥ng b√°o ƒë√≥, KH√îNG ƒë·ªïi th√†nh format b∆∞·ªõc

üìã V√ç D·ª§ TR·∫¢ L·ªúI T·ª∞ NHI√äN (CONVERSATIONAL):

**V√≠ d·ª• 1: H·ªßy v√≠ VNPT Money** (NG·ªÆ C·∫¢NH c√≥ "B∆∞·ªõc 1, 2, 3" ‚Üí VI·∫æT L·∫†I t·ª± nhi√™n):
```
ƒê·ªÉ h·ªßy v√≠ VNPT Money, b·∫°n c·∫ßn l√†m m·ªôt v√†i vi·ªác nh√©:

ƒê·∫ßu ti√™n, h√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ ng·∫Øt k·∫øt n·ªëi v·ªõi t·∫•t c·∫£ t√†i kho·∫£n ng√¢n h√†ng tr√™n v√≠ VNPT Money.

Ti·∫øp theo, n·∫øu v√≠ c·ªßa b·∫°n c√≤n s·ªë d∆∞, b·∫°n n√™n s·ª≠ d·ª•ng h·∫øt ho·∫∑c chuy·ªÉn ra ng√¢n h√†ng tr∆∞·ªõc nh√©. Ngo√†i ra, n·∫øu c√≥ kho·∫£n n·ª£ n√†o, b·∫°n c≈©ng c·∫ßn thanh to√°n h·∫øt lu√¥n.

Sau khi ho√†n t·∫•t nh·ªØng b∆∞·ªõc tr√™n, b·∫°n li√™n h·ªá v·ªõi b·ªô ph·∫≠n h·ªó tr·ª£ v√† cung c·∫•p c√°c th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ h·ªç x·ª≠ l√Ω vi·ªác h·ªßy v√≠ cho b·∫°n.

‚ö†Ô∏è L∆∞u √Ω: Khi v√≠ ƒë√£ ƒë∆∞·ª£c h·ªßy, b·∫°n s·∫Ω kh√¥ng th·ªÉ kh√¥i ph·ª•c l·∫°i ƒë∆∞·ª£c nh√©.
```

**V√≠ d·ª• 2: R√∫t ti·ªÅn b·ªã m·∫•t ph√≠** (NG·ªÆ C·∫¢NH KH√îNG c√≥ "B∆∞·ªõc" ‚Üí Tr·∫£ l·ªùi t·ª± nhi√™n):
```
Khi b·∫°n r√∫t ti·ªÅn t·ª´ v√≠ VNPT Pay, b·∫°n s·∫Ω b·ªã t√≠nh kho·∫£n ph√≠ theo ch√≠nh s√°ch c·ªßa VNPT Pay. ƒê√¢y l√† ph√≠ d·ªãch v·ª• chu·∫©n, kh√¥ng ph·∫£i l·ªói nh√©.

N·∫øu b·∫°n mu·ªën bi·∫øt r√µ h∆°n v·ªÅ m·ª©c ph√≠, b·∫°n c√≥ th·ªÉ ki·ªÉm tra trong ph·∫ßn "Bi·ªÉu ph√≠" tr√™n ·ª©ng d·ª•ng ho·∫∑c li√™n h·ªá Hotline 1900 8198 ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n.
```

**V√≠ d·ª• 3: N·∫°p ti·ªÅn t·ª´ ng√¢n h√†ng li√™n k·∫øt** (NG·ªÆ C·∫¢NH c√≥ "B∆∞·ªõc" ‚Üí VI·∫æT L·∫†I t·ª± nhi√™n):
```
N·∫°p ti·ªÅn t·ª´ ng√¢n h√†ng li√™n k·∫øt r·∫•t ƒë∆°n gi·∫£n th√¥i b·∫°n:

ƒê·∫ßu ti√™n, b·∫°n ch·ªçn "N·∫°p ti·ªÅn" tr√™n m√†n h√¨nh ch√≠nh.

Ti·∫øp theo, nh·∫≠p s·ªë ti·ªÅn b·∫°n mu·ªën n·∫°p v√†o, r·ªìi ·∫•n "X√°c nh·∫≠n".

Sau ƒë√≥, h·ªá th·ªëng s·∫Ω g·ª≠i m√£ OTP ƒë·∫øn s·ªë ƒëi·ªán tho·∫°i c·ªßa b·∫°n. B·∫°n ch·ªâ c·∫ßn nh·∫≠p m√£ OTP n√†y ƒë·ªÉ ho√†n t·∫•t n·∫°p ti·ªÅn l√† xong!

üí° Ti·ªÅn s·∫Ω ƒë∆∞·ª£c chuy·ªÉn v√†o v√≠ c·ªßa b·∫°n ngay l·∫≠p t·ª©c sau khi x√°c nh·∫≠n th√†nh c√¥ng.
```

**Completion message** (ƒë√£ ho√†n th√†nh T·∫§T C·∫¢ c√°c b∆∞·ªõc - GI·ªÆ NGUY√äN):
```
‚úÖ B·∫°n ƒë√£ ho√†n th√†nh t·∫•t c·∫£ 5 b∆∞·ªõc!

N·∫øu b·∫°n v·∫´n g·∫∑p v·∫•n ƒë·ªÅ ho·∫∑c c·∫ßn h·ªó tr·ª£ th√™m, vui l√≤ng li√™n h·ªá:
üìû Hotline: 1900 8198 (24/7)
‚úâÔ∏è Email: hotro@vnptmoney.vn
```
(L∆∞u √Ω: GI·ªÆ NGUY√äN to√†n b·ªô completion message, KH√îNG format l·∫°i)

‚ö†Ô∏è CRITICAL - QUY T·∫ÆC VI·∫æT:
- D√ôNG chuy·ªÉn ti·∫øp t·ª± nhi√™n: "ƒê·∫ßu ti√™n...", "Ti·∫øp theo...", "Sau ƒë√≥...", "Cu·ªëi c√πng..."
- TR√ÅNH "B∆∞·ªõc 1, B∆∞·ªõc 2, B∆∞·ªõc 3" tr·ª´ khi NG·ªÆ C·∫¢NH g·ªëc B·∫ÆT BU·ªòC ph·∫£i c√≥
- TH√äM ƒë·ªông vi√™n: "r·∫•t ƒë∆°n gi·∫£n th√¥i", "ƒë·ª´ng lo nh√©", "d·ªÖ d√†ng", "ch·ªâ c·∫ßn..."
- D√ôNG ng√¥n ng·ªØ th√¢n m·∫≠t: "b·∫°n", "nh√©", "nha", "c·ªßa b·∫°n"
- M·ªñI ƒëo·∫°n XU·ªêNG D√íNG ƒë·ªÉ d·ªÖ ƒë·ªçc (m·ªói d√≤ng t·ªëi ƒëa 80-100 k√Ω t·ª±)
- GI·ªÆ kho·∫£ng tr·∫Øng gi·ªØa c√°c ph·∫ßn ƒë·ªÉ tho√°ng
- CH·ªà bao g·ªìm "L∆∞u √Ω" n·∫øu TR·ª∞C TI·∫æP li√™n quan ƒë·∫øn c√¢u h·ªèi
- N·∫øu l√† COMPLETION MESSAGE: GI·ªÆ NGUY√äN, kh√¥ng format l·∫°i

{continuation_instruction}üìö NG·ªÆ C·∫¢NH (ƒê·ªô tin c·∫≠y: {confidence:.0%}):
{context}

‚ùì C√ÇU H·ªéI:
"{user_message}"

üí¨ TR·∫¢ L·ªúI (format d·ªÖ ƒë·ªçc nh∆∞ v√≠ d·ª• - m·ªói b∆∞·ªõc/bullet xu·ªëng d√≤ng):
"""

        return prompt

    def _call_openai(self, prompt: str) -> str:
        """Call OpenAI API"""
        try:
            # System message with formatting instructions
            system_message = """B·∫°n l√† VNPT Assistant - tr·ª£ l√Ω ·∫£o c·ªßa VNPT Money, n√≥i chuy·ªán T·ª∞ NHI√äN nh∆∞ m·ªôt ng∆∞·ªùi t∆∞ v·∫•n vi√™n th√¢n thi·ªán.

NHI·ªÜM V·ª§ CH√çNH:
- Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH
- Vi·∫øt theo phong c√°ch T·ª∞ NHI√äN, TH√ÇN THI·ªÜN nh∆∞ ƒëang t∆∞ v·∫•n 1-1 cho kh√°ch h√†ng
- Format D·ªÑ ƒê·ªåC: M·ªói d√≤ng t·ªëi ƒëa 80-100 k√Ω t·ª±
- KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin

QUY T·∫ÆC VI·∫æT T·ª∞ NHI√äN (‚ö†Ô∏è QUAN TR·ªåNG NH·∫§T):
- TR√ÅNH ng√¥n ng·ªØ c·ª©ng nh·∫Øc ki·ªÉu bot: KH√îNG d√πng "B∆∞·ªõc 1, B∆∞·ªõc 2, B∆∞·ªõc 3" TR·ª™ KHI ng·ªØ c·∫£nh G·ªêC c√≥ s·∫µn
- S·ª¨ D·ª§NG chuy·ªÉn ti·∫øp t·ª± nhi√™n: "ƒê·∫ßu ti√™n...", "Ti·∫øp theo...", "Sau ƒë√≥...", "Cu·ªëi c√πng..."
- TH√äM ƒë·ªông vi√™n v√† c·∫£m x√∫c: "ƒë·ª´ng lo nh√©", "r·∫•t ƒë∆°n gi·∫£n th√¥i", "d·ªÖ d√†ng", "ch·ªâ c·∫ßn..."
- D√ôNG ng√¥n ng·ªØ th√¢n m·∫≠t: "b·∫°n", "m√¨nh", "nh√©", "nha", "c·ªßa b·∫°n"
- N√ìI nh∆∞ ƒëang t∆∞ v·∫•n tr·ª±c ti·∫øp: M∆∞·ª£t m√†, th·∫•u hi·ªÉu, kh√¥ng c·ª©ng nh·∫Øc

V√ç D·ª§ T·ªêT (T·ª∞ NHI√äN):
‚ùå TR√ÅNH: "B∆∞·ªõc 1: ƒê·∫£m b·∫£o kh√¥ng c√≤n li√™n k·∫øt. B∆∞·ªõc 2: S·ª≠ d·ª•ng h·∫øt s·ªë d∆∞. B∆∞·ªõc 3: Thanh to√°n d∆∞ n·ª£."
‚úÖ VI·∫æT: "ƒê·∫ßu ti√™n, h√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ ng·∫Øt k·∫øt n·ªëi v·ªõi t·∫•t c·∫£ t√†i kho·∫£n ng√¢n h√†ng. Ti·∫øp theo, n·∫øu v√≠ c√≤n s·ªë d∆∞, b·∫°n n√™n s·ª≠ d·ª•ng h·∫øt ho·∫∑c chuy·ªÉn ra ng√¢n h√†ng tr∆∞·ªõc nh√©."

FORMAT:
- M·ªñI ƒëo·∫°n XU·ªêNG D√íNG ƒë·ªÉ d·ªÖ ƒë·ªçc (m·ªói d√≤ng t·ªëi ƒëa 80-100 k√Ω t·ª±)
- KH√îNG d√πng bullet points (‚Ä¢)
- GI·ªÆ kho·∫£ng tr·∫Øng gi·ªØa c√°c ƒëo·∫°n ƒë·ªÉ tho√°ng
- C√ì TH·ªÇ th√™m icon th√¢n thi·ªán (‚ö†Ô∏è üí° ‚úÖ ‚ùå üìû) khi ph√π h·ª£p
- CH·ªà bao g·ªìm "L∆∞u √Ω" n·∫øu TR·ª∞C TI·∫æP li√™n quan ƒë·∫øn c√¢u h·ªèi
- KH√îNG th√™m: "Ch√†o b·∫°n", "C√¢u h·ªèi li√™n quan", ho·∫∑c "L∆∞u √Ω" t·ª´ FAQ kh√¥ng li√™n quan
- ‚ö†Ô∏è COMPLETION MESSAGE: N·∫øu NG·ªÆ C·∫¢NH c√≥ th√¥ng b√°o ho√†n th√†nh (‚úÖ, "ƒë√£ ho√†n th√†nh t·∫•t c·∫£", "Hotline: 1900"), GI·ªÆ NGUY√äN to√†n b·ªô, KH√îNG format l·∫°i
"""

            # Call OpenAI API
            response = self.llm_client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=config.LLM_TEMPERATURE,
                max_tokens=config.LLM_MAX_TOKENS
            )

            answer = response.choices[0].message.content.strip()

            # Post-process to improve formatting
            answer = self._format_answer_for_readability(answer)

            return answer

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise

    def _format_answer_for_readability(self, answer: str) -> str:
        """
        Post-process answer to enforce better formatting:
        - Break long lines with comma-separated actions
        - Add spacing between steps
        """
        import re

        lines = answer.split('\n')
        formatted_lines = []

        for line in lines:
            # Check if this is a step line (B∆∞·ªõc X: ...)
            step_match = re.match(r'(B∆∞·ªõc\s+\d+:\s*)(.+)', line)

            if step_match:
                step_label = step_match.group(1)  # "B∆∞·ªõc 1: "
                step_content = step_match.group(2)  # The actual content

                # If content has multiple comma-separated clauses and is long (>80 chars)
                # OR has 3+ commas (lots of actions)
                comma_count = step_content.count(',')

                if comma_count >= 2 or (comma_count >= 1 and len(step_content) > 80):
                    # Split by comma
                    parts = [p.strip() for p in step_content.split(',')]

                    # First part goes on same line as "B∆∞·ªõc X:"
                    formatted_lines.append(f"{step_label}{parts[0]}")

                    # Rest go on separate lines
                    for part in parts[1:]:
                        if part:  # Skip empty parts
                            # Capitalize if starts with lowercase
                            formatted_part = part[0].upper() + part[1:] if part and part[0].islower() else part
                            formatted_lines.append(formatted_part)

                    # Add blank line after step for spacing
                    formatted_lines.append('')
                else:
                    # Step is short enough, keep as is
                    formatted_lines.append(line)
                    # Add blank line after step for spacing
                    if len(step_content) > 0:
                        formatted_lines.append('')
            else:
                # Not a step line, keep as is
                formatted_lines.append(line)

        # Remove trailing blank lines
        while formatted_lines and not formatted_lines[-1].strip():
            formatted_lines.pop()

        return '\n'.join(formatted_lines)

    def _generate_template_response(self, rag_result: Dict) -> str:
        """Generate template response without LLM"""

        if rag_result["status"] != "success":
            return """Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n trong c∆° s·ªü d·ªØ li·ªáu.

ƒê·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·ªët h∆°n, b·∫°n c√≥ th·ªÉ:
üìû G·ªçi Hotline: 1900 8198 (24/7)
‚úâÔ∏è Email: hotro@vnptmoney.vn
üåê Truy c·∫≠p: https://vnptpay.vn

Ch√∫ng t√¥i lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!"""

        # Extract information
        answer = rag_result.get("answer", "")
        alternative_actions = rag_result.get("alternative_actions", [])
        related_questions = rag_result.get("related_questions", [])
        related_entities = rag_result.get("related_entities", {})
        confidence = rag_result.get("confidence", 0)

        # Build response
        response_parts = []

        # Main answer with confidence indicator
        if confidence >= 0.8:
            response_parts.append(f"{answer}")
        else:
            response_parts.append(f"D·ª±a tr√™n th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c:\n\n{answer}")

        # Alternative actions
        if alternative_actions:
            response_parts.append("\nüí° **Ph∆∞∆°ng √°n thay th·∫ø:**")
            for i, alt in enumerate(alternative_actions, 1):
                response_parts.append(f"{i}. {alt['action']}: {alt['reason']}")

        # Additional info from entities
        if related_entities:
            notes = []
            if related_entities.get("errors"):
                notes.append(f"‚ö†Ô∏è L·ªói li√™n quan: {', '.join(related_entities['errors'])}")
            if related_entities.get("features"):
                notes.append(f"‚ú® T√≠nh nƒÉng: {', '.join(related_entities['features'])}")

            if notes:
                response_parts.append("\n" + "\n".join(notes))

        # Related questions
        if related_questions:
            response_parts.append("\n‚ùì **B·∫°n c√≥ th·ªÉ quan t√¢m:**")
            for i, rq in enumerate(related_questions[:3], 1):
                response_parts.append(f"{i}. {rq['question']}")

        # Footer
        response_parts.append("\n---")
        response_parts.append("N·∫øu c·∫ßn h·ªó tr·ª£ th√™m, vui l√≤ng li√™n h·ªá Hotline: 1900 8198")

        return "\n".join(response_parts)

    def get_conversation_history(self) -> List[Dict]:
        """Get conversation history"""
        return self.conversation_history

    def clear_history(self):
        """Clear conversation history and context"""
        self.conversation_history = []
        self.context_manager.clear_context()
        logger.info("Conversation history and context cleared")

    def get_chat_statistics(self) -> Dict:
        """Get chatbot statistics with context info"""
        context_summary = self.context_manager.get_summary()
        return {
            "total_conversations": len(self.conversation_history),
            "llm_enabled": self.llm is not None,
            "llm_provider": config.LLM_PROVIDER,
            "cache_size": len(self.rag_engine.cache),
            "context_turns": context_summary.get("num_turns", 0),
            "current_topic": context_summary.get("current_topic"),
            "has_active_context": context_summary.get("has_active_context", False)
        }


# ============================================
# INTERACTIVE CHAT
# ============================================

def interactive_chat():
    """Run interactive chatbot session"""
    print("=" * 60)
    print("VNPT Money GraphRAG Chatbot")
    print("=" * 60)
    print("G√µ 'exit' ho·∫∑c 'quit' ƒë·ªÉ tho√°t")
    print("G√µ 'clear' ƒë·ªÉ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i")
    print("G√µ 'stats' ƒë·ªÉ xem th·ªëng k√™")
    print("=" * 60)

    chatbot = GraphRAGChatbot()

    while True:
        try:
            user_input = input("\n\nB·∫°n: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['exit', 'quit', 'tho√°t']:
                print("\nC·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng VNPT Money Chatbot. H·∫πn g·∫∑p l·∫°i!")
                break

            if user_input.lower() == 'clear':
                chatbot.clear_history()
                print("\nƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i.")
                continue

            if user_input.lower() == 'stats':
                stats = chatbot.get_chat_statistics()
                print(f"\nTh·ªëng k√™:")
                print(f"  - S·ªë l∆∞·ª£t h·ªôi tho·∫°i: {stats['total_conversations']}")
                print(f"  - LLM: {stats['llm_provider']} ({'Enabled' if stats['llm_enabled'] else 'Disabled'})")
                print(f"  - Cache size: {stats['cache_size']}")
                continue

            # Get response
            response = chatbot.chat(user_input)

            print(f"\nVNPT Money Bot: {response}")

        except KeyboardInterrupt:
            print("\n\nC·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng VNPT Money Chatbot. H·∫πn g·∫∑p l·∫°i!")
            break
        except Exception as e:
            logger.error(f"Error in chat loop: {e}")
            print(f"\nXin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i.")


if __name__ == "__main__":
    interactive_chat()
