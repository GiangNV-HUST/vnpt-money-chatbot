# -*- coding: utf-8 -*-
"""
So s√°nh 3 ph∆∞∆°ng ph√°p extraction:
1. Pure Regex (Before)
2. Pattern-First Hybrid (Old)
3. LLM-First Hybrid (New)
"""
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from enhanced_entity_extractor import EnhancedEntityExtractor
import config
import time

print("=" * 80)
print("COMPARISON: Pure Regex vs Pattern-First vs LLM-First")
print("=" * 80)

# Test cases - c√°c truy v·∫•n th·ª±c t·∫ø
test_cases = [
    "Chuy·ªÉn ti·ªÅn t·ª´ Vietinbank sang VNPT Money c·∫ßn ƒëi·ªÅu ki·ªán g√¨?",
    "M·ªü t√†i kho·∫£n VNPT Money c·∫ßn CCCD kh√¥ng?",
    "L√†m sao thanh to√°n h√≥a ƒë∆°n ƒëi·ªán qua app?",
    "R√∫t ti·ªÅn v·ªÅ t√†i kho·∫£n ng√¢n h√†ng",
    "App c√≥ h·ªó tr·ª£ QR code thanh to√°n kh√¥ng?",
]

extractor = EnhancedEntityExtractor()

results = {
    'pure_regex': [],
    'pattern_first': [],
    'llm_first': []
}

# Test 1: Pure Regex (Before - No LLM at all)
print("\n" + "=" * 80)
print("TEST 1: PURE REGEX (BEFORE - No LLM)")
print("=" * 80)

config.ENABLE_LLM_FALLBACK = False
config.USE_LLM_FIRST_STRATEGY = False

for i, query in enumerate(test_cases, 1):
    print(f"\n{i}. {query}")
    print("-" * 80)

    start = time.time()
    entities, confidence = extractor.extract_with_confidence(query)
    elapsed = time.time() - start

    results['pure_regex'].append({
        'query': query,
        'entities': entities,
        'confidence': confidence,
        'time': elapsed
    })

    # Count entities
    entity_count = sum(len(v) for v in entities.values() if isinstance(v, list))

    print(f"Confidence: {confidence:.0%}")
    print(f"Time: {elapsed*1000:.0f}ms")
    print(f"Entities found: {entity_count}")
    for entity_type, values in entities.items():
        if values and isinstance(values, list):
            print(f"  {entity_type}: {values}")

# Test 2: Pattern-First Hybrid (Old - LLM fallback only)
print("\n" + "=" * 80)
print("TEST 2: PATTERN-FIRST HYBRID (OLD - LLM Fallback)")
print("=" * 80)

config.ENABLE_LLM_FALLBACK = True
config.USE_LLM_FIRST_STRATEGY = False

for i, query in enumerate(test_cases, 1):
    print(f"\n{i}. {query}")
    print("-" * 80)

    start = time.time()
    entities, confidence = extractor.extract_with_confidence(query)
    elapsed = time.time() - start

    results['pattern_first'].append({
        'query': query,
        'entities': entities,
        'confidence': confidence,
        'time': elapsed
    })

    # Count entities
    entity_count = sum(len(v) for v in entities.values() if isinstance(v, list))

    print(f"Confidence: {confidence:.0%}")
    print(f"Time: {elapsed*1000:.0f}ms")
    print(f"Entities found: {entity_count}")
    for entity_type, values in entities.items():
        if values and isinstance(values, list):
            print(f"  {entity_type}: {values}")

# Test 3: LLM-First Hybrid (New - Always LLM)
print("\n" + "=" * 80)
print("TEST 3: LLM-FIRST HYBRID (NEW - Always LLM)")
print("=" * 80)

config.ENABLE_LLM_FALLBACK = True
config.USE_LLM_FIRST_STRATEGY = True

for i, query in enumerate(test_cases, 1):
    print(f"\n{i}. {query}")
    print("-" * 80)

    start = time.time()
    entities, confidence = extractor.extract_with_confidence(query)
    elapsed = time.time() - start

    results['llm_first'].append({
        'query': query,
        'entities': entities,
        'confidence': confidence,
        'time': elapsed
    })

    # Count entities
    entity_count = sum(len(v) for v in entities.values() if isinstance(v, list))

    print(f"Confidence: {confidence:.0%}")
    print(f"Time: {elapsed*1000:.0f}ms")
    print(f"Entities found: {entity_count}")
    for entity_type, values in entities.items():
        if values and isinstance(values, list):
            print(f"  {entity_type}: {values}")

# Comparison Summary
print("\n" + "=" * 80)
print("COMPARISON SUMMARY")
print("=" * 80)

# Calculate stats
def calc_stats(results_list):
    total_time = sum(r['time'] for r in results_list)
    avg_time = total_time / len(results_list)
    avg_conf = sum(r['confidence'] for r in results_list) / len(results_list)
    total_entities = sum(
        sum(len(v) for v in r['entities'].values() if isinstance(v, list))
        for r in results_list
    )
    avg_entities = total_entities / len(results_list)
    return avg_time, avg_conf, avg_entities

pure_avg_time, pure_avg_conf, pure_avg_entities = calc_stats(results['pure_regex'])
pattern_avg_time, pattern_avg_conf, pattern_avg_entities = calc_stats(results['pattern_first'])
llm_avg_time, llm_avg_conf, llm_avg_entities = calc_stats(results['llm_first'])

print(f"\nüìä Overall Performance Metrics:")
print(f"{'Metric':<30} | Pure Regex | Pattern-First | LLM-First")
print("-" * 80)
print(f"{'Avg Response Time':<30} | {pure_avg_time*1000:>8.0f}ms | {pattern_avg_time*1000:>11.0f}ms | {llm_avg_time*1000:>9.0f}ms")
print(f"{'Avg Confidence':<30} | {pure_avg_conf:>9.0%} | {pattern_avg_conf:>12.0%} | {llm_avg_conf:>9.0%}")
print(f"{'Avg Entities/Query':<30} | {pure_avg_entities:>10.1f} | {pattern_avg_entities:>13.1f} | {llm_avg_entities:>9.1f}")

# Query-by-query comparison
print(f"\nüìã Query-by-Query Comparison:")
print("-" * 80)

for i in range(len(test_cases)):
    query = test_cases[i]
    pure = results['pure_regex'][i]
    pattern = results['pattern_first'][i]
    llm = results['llm_first'][i]

    print(f"\n{i+1}. {query[:60]}...")

    # Count entities
    pure_count = sum(len(v) for v in pure['entities'].values() if isinstance(v, list))
    pattern_count = sum(len(v) for v in pattern['entities'].values() if isinstance(v, list))
    llm_count = sum(len(v) for v in llm['entities'].values() if isinstance(v, list))

    print(f"   Pure Regex:      {pure_count} entities, {pure['confidence']:>3.0%} conf, {pure['time']*1000:>5.0f}ms")
    print(f"   Pattern-First:   {pattern_count} entities, {pattern['confidence']:>3.0%} conf, {pattern['time']*1000:>5.0f}ms")
    print(f"   LLM-First:       {llm_count} entities, {llm['confidence']:>3.0%} conf, {llm['time']*1000:>5.0f}ms")

    # Find what LLM-First found that Pure Regex missed
    llm_types = set(k for k, v in llm['entities'].items() if v and isinstance(v, list))
    pure_types = set(k for k, v in pure['entities'].items() if v and isinstance(v, list))

    new_types = llm_types - pure_types
    if new_types:
        print(f"   ‚úÖ LLM-First found extra: {', '.join(new_types)}")
        for entity_type in new_types:
            print(f"      ‚Üí {entity_type}: {llm['entities'][entity_type]}")

print(f"\n{'=' * 80}")
print("CONCLUSION")
print(f"{'=' * 80}")

print(f"""
D·ª±a tr√™n {len(test_cases)} queries test:

1. **Pure Regex (Before)**:
   - Avg Confidence: {pure_avg_conf:.0%} ‚ö†Ô∏è
   - Avg Time: {pure_avg_time*1000:.0f}ms ‚ö°
   - Avg Entities: {pure_avg_entities:.1f}
   - ‚ùå Thi·∫øu semantic understanding
   - ‚ùå B·ªè l·ª° nhi·ªÅu entities

2. **Pattern-First Hybrid (Old)**:
   - Avg Confidence: {pattern_avg_conf:.0%}
   - Avg Time: {pattern_avg_time*1000:.0f}ms
   - Avg Entities: {pattern_avg_entities:.1f}
   - ‚ö†Ô∏è LLM ch·ªâ trigger khi confidence th·∫•p
   - ‚ö†Ô∏è V·∫´n c√≥ th·ªÉ b·ªè l·ª° entities

3. **LLM-First Hybrid (New - RECOMMENDED)**:
   - Avg Confidence: {llm_avg_conf:.0%} ‚úÖ
   - Avg Time: {llm_avg_time*1000:.0f}ms
   - Avg Entities: {llm_avg_entities:.1f} ‚úÖ
   - ‚úÖ Best accuracy
   - ‚úÖ Consistent confidence
   - ‚úÖ Semantic understanding
   - ‚ö†Ô∏è Slower, more expensive (acceptable per user requirement)

**K·∫øt lu·∫≠n**: LLM-First t·ªët h∆°n r√µ r·ªát v·ªÅ accuracy v√† completeness!
Improvement: {(llm_avg_conf/pure_avg_conf - 1)*100:+.0f}% confidence, {(llm_avg_entities/pure_avg_entities - 1)*100:+.0f}% entities
""")

# Reset to LLM-First
config.ENABLE_LLM_FALLBACK = True
config.USE_LLM_FIRST_STRATEGY = True
